{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f6388b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T05:24:08.096008Z",
     "start_time": "2023-09-29T05:24:06.338124Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/travers/miniconda3/envs/timewarpvae/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import timewarp_lib.vector_timewarpers as vtw\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81f690e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T05:24:08.118261Z",
     "start_time": "2023-09-29T05:24:08.098036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750, 200, 2)\n"
     ]
    }
   ],
   "source": [
    "tau = 1\n",
    "yzero = 0\n",
    "\n",
    "# arbitrary?\n",
    "alpha_z = 10\n",
    "beta_z = alpha_z/4\n",
    "\n",
    "N = 50\n",
    "sigma = 2/N\n",
    "\n",
    "def phase_func(t):\n",
    "    phasefactor = -np.log(0.01)\n",
    "    return np.exp(-t * phasefactor)\n",
    "\n",
    "cs = phase_func(np.linspace(0,1,N,endpoint=True))\n",
    "\n",
    "fulldat = np.load(\"../data/trainTest2DLetterACache.npz\")\n",
    "print(fulldat[\"train\"].shape)\n",
    "dat = fulldat[\"train\"]\n",
    "numdims = dat.shape[2]\n",
    "numts = dat.shape[1]\n",
    "numtrajs = dat.shape[0]\n",
    "\n",
    "\n",
    "ts = np.linspace(0,1,numts)\n",
    "xs = phase_func(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ef41c04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T05:24:08.123377Z",
     "start_time": "2023-09-29T05:24:08.119680Z"
    }
   },
   "outputs": [],
   "source": [
    "# Yeah, i don't _really_ get it, but an original Ijspeert paper\n",
    "# (and the Learning Parametric Dynamic Movement... paper)\n",
    "# have the forcing term affect velocity, not acceleration\n",
    "def numeric_integration(ydemos, ts, tau, g, alpha_z, beta_z):\n",
    "    step_size = 0.00001\n",
    "    i = 0\n",
    "    t = ts[i]\n",
    "    i += 1\n",
    "    z = 0\n",
    "    zs = []\n",
    "    zs.append(z)\n",
    "    while i < len(ts):\n",
    "        while i < len(ts) and t < ts[i]:\n",
    "            interp_frac = (t - ts[i-1])/(ts[i] - ts[i-1])\n",
    "            y = (1-interp_frac) * ydemos[i-1] + interp_frac * ydemos[i]\n",
    "            z += alpha_z * (beta_z * (g - y) - z)/tau * step_size\n",
    "            t += step_size\n",
    "        zs.append(z)\n",
    "        i += 1\n",
    "    return np.array(zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fd9ae2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T05:24:08.127606Z",
     "start_time": "2023-09-29T05:24:08.125028Z"
    }
   },
   "outputs": [],
   "source": [
    "# basisphis, targetfunction, xs are all evaluated at ts\n",
    "def fit_target_i(i, basisphis, targetfunction, xs, yzero, g):\n",
    "    s = xs * (g - yzero)\n",
    "    gamma = np.diag(basisphis[i])\n",
    "    # equation 2.14 from Dynamical Movement Primitives: Learning Attractor Models for Motor Behaviors\n",
    "    numerator = s @ gamma @ targetfunction\n",
    "    denominator = s @ gamma @ s\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80bfd633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T05:24:08.696277Z",
     "start_time": "2023-09-29T05:24:08.692491Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate(fitted_f, ts, g, alpha_z, beta_z):\n",
    "    step_size = 0.00001\n",
    "    i = 0\n",
    "    t = ts[i]\n",
    "    ys = [] # position\n",
    "    zs = [] # velocity\n",
    "    y = 0\n",
    "    z = 0\n",
    "    t = 0\n",
    "    while i < len(ts):\n",
    "        while i < len(ts) and t < ts[i]:\n",
    "            interp_frac = (t - ts[i-1])/(ts[i] - ts[i-1])\n",
    "            f = (1-interp_frac) * fitted_f[i-1] + interp_frac * fitted_f[i]\n",
    "            z += alpha_z * (beta_z * (g - y) - z)/tau * step_size\n",
    "            y += (z + f)/tau * step_size\n",
    "            t += step_size\n",
    "        ys.append(y)\n",
    "        zs.append(z)\n",
    "        i += 1\n",
    "    return (np.array(ys), np.array(zs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440f8fe8",
   "metadata": {},
   "source": [
    "## Compute DTW parameterization of each training trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2e6f557",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T05:24:09.711772Z",
     "start_time": "2023-09-29T05:24:09.704544Z"
    }
   },
   "outputs": [],
   "source": [
    "# traj should be 2d\n",
    "def compute_dtw_parameterization(traj):\n",
    "    assert len(traj.shape) == 2, \"traj should be 2d...just send in one\"\n",
    "    start_offset_0, start_offset_1 = traj[0]\n",
    "    ydemos = traj-traj[0:1]\n",
    "    result_dictionary = {}\n",
    "    for dim in range(2):\n",
    "        ydemo = ydemos[:,dim]\n",
    "        ydemoprime = (ydemo[2:]-ydemo[:-2])/(ts[1]-ts[0])/2\n",
    "        ydemoprime = np.concatenate(((ydemo[1:2]-ydemo[:1])/(ts[1]-ts[0]),ydemoprime,(ydemo[-1:]-ydemo[-2:-1])/(ts[1]-ts[0])))\n",
    "        yzero = ydemo[0]\n",
    "        g = ydemo[-1]\n",
    "        basisphis = np.array([np.exp(-(phase_func(ts) - c)**2/((sigma * c)**2)) for c in cs])\n",
    "        zdemo = numeric_integration(ydemo, ts, tau, g, alpha_z, beta_z)\n",
    "        ftarget = tau * ydemoprime - zdemo\n",
    "        ws = np.array([fit_target_i(i, basisphis, ftarget, xs, yzero, g) for i in range(len(basisphis))])\n",
    "        result_dictionary[f\"ws_{dim}\"] = ws\n",
    "        result_dictionary[f\"g_{dim}\"] = g\n",
    "    result_dictionary[f\"start_offset_0\"] = start_offset_0\n",
    "    result_dictionary[f\"start_offset_1\"] = start_offset_1\n",
    "    return result_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7dda4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "258dc12c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T05:24:10.467111Z",
     "start_time": "2023-09-29T05:24:10.460677Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_parameter_vector(parameter_vector):\n",
    "    ws_0 = parameter_vector[:N]\n",
    "    ws_1 = parameter_vector[N:2*N]\n",
    "    start_offset_0,start_offset_1,g_0,g_1 = parameter_vector[2*N:]\n",
    "    basisphis = np.array([np.exp(-(phase_func(ts) - c)**2/((sigma * c)**2)) for c in cs])\n",
    "    \n",
    "    positions = []\n",
    "    for g,ws,start_offset in [(g_0,ws_0,start_offset_0), (g_1,ws_1,start_offset_1)]:\n",
    "        yzero=0 # centered training data all starts at zero\n",
    "        fitted_f = np.einsum(\"it,i->t\",basisphis,ws)/np.einsum(\"it->t\",basisphis) * xs * (g-yzero)\n",
    "        # ys = position, zs = velocity\n",
    "        ys,zs = simulate(fitted_f,ts,g,alpha_z,beta_z)\n",
    "        positions.append(ys + start_offset)\n",
    "    positions=np.array(positions).T\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29558dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a790d70e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T05:24:11.288131Z",
     "start_time": "2023-09-29T05:24:11.281039Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_dmp_parameter_model(training_data, latent_dim,scale_last_four_dims=1):\n",
    "    num_trajs, num_channels = training_data.shape\n",
    "    print(f\"we have num_trajs:{num_trajs}, num_channels:{num_channels}\")\n",
    "    flattened_trajs = np.copy(training_data)\n",
    "    \n",
    "    flattened_trajs[:,-4:] = flattened_trajs[:,-4:] * scale_last_four_dims\n",
    "\n",
    "    # compute and remove the mean trajectory (mean trajectory is of shape (num_timesteps * num_channels))\n",
    "    mean_traj = np.mean(flattened_trajs, axis=0)\n",
    "    centered_trajs = flattened_trajs - mean_traj[np.newaxis,:]\n",
    "\n",
    "    # compute the PCA model using SVD\n",
    "    u, s, vt = np.linalg.svd(centered_trajs)\n",
    "\n",
    "    # keep only the first latent_dim number of dimensions\n",
    "    if len(s) < latent_dim:\n",
    "        raise Exception(f\"You can't build a model of dimension {latent_dim} on a dataset with dimensionality {len(s)}\")\n",
    "\n",
    "    # the singular values written in matrix form\n",
    "    smat = np.diag(s[:latent_dim])\n",
    "    smatinv = np.diag(1./s[:latent_dim])\n",
    "    # the first latent_dim directions of variation\n",
    "    basis_vectors = vt[:latent_dim,:]\n",
    "    # given a trajectory, use this to find its value in the latent space \n",
    "    # note that the dimensions of the embedding matrix are (latent_dim, nt*nc)\n",
    "    embedding_matrix = smatinv @ basis_vectors \n",
    "    # note that we include the s factor here so that our latent space is roughly the unit gaussian ball :brain:\n",
    "    # note further that the dimensions of the reconstruction matrix are also (latent_dim, nt*nc)\n",
    "    reconstruction_matrix = smat @ basis_vectors\n",
    "\n",
    "    return {\n",
    "      \"num_trajs\" : num_trajs, \n",
    "      \"num_channels\" : num_channels,\n",
    "      \"latent_dim\" : latent_dim,\n",
    "      \"mean_traj\" : mean_traj,\n",
    "      \"embedding_matrix\" : embedding_matrix,\n",
    "      \"reconstruction_matrix\" : reconstruction_matrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f765e9ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T05:24:12.126045Z",
     "start_time": "2023-09-29T05:24:12.118973Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a ModelApplier object based purely on a\n",
    "# directory containing model information\n",
    "class ModelApplier(object):\n",
    "    def __init__(self, modeldata):\n",
    "        self.num_trajs = modeldata[\"num_trajs\"]\n",
    "        self.num_channels = modeldata[\"num_channels\"]\n",
    "        self.latent_dim = modeldata[\"latent_dim\"]\n",
    "        self.mean_traj = modeldata[\"mean_traj\"]\n",
    "        self.embedding_matrix = modeldata[\"embedding_matrix\"]\n",
    "        self.reconstruction_matrix = modeldata[\"reconstruction_matrix\"]\n",
    "\n",
    "    # The input data should be of the shape\n",
    "    # (num_apply_trajs, apply_latent_dim)\n",
    "    def apply(self, data):\n",
    "        num_apply_trajs, apply_latent_dim = data.shape\n",
    "        if apply_latent_dim != self.latent_dim:\n",
    "              raise Exception(f\"The number of latent dim coordinates given: {apply_latent_dim} was not the same as the {self.latent_dim} expected by the model\")\n",
    "\n",
    "        # compute the linear combination of basis vectors \n",
    "        traj_offset_vectors = data @ self.reconstruction_matrix\n",
    "        mean_vector = self.mean_traj.reshape(1,self.num_channels)\n",
    "        # add back the mean vector\n",
    "        traj_vectors = traj_offset_vectors + mean_vector\n",
    "        # reshape to the official standard expected shape\n",
    "        # (num_apply_trajs, num_timesteps, num_channels)\n",
    "        result_trajs = traj_vectors.reshape(num_apply_trajs, self.num_channels)\n",
    "        return result_trajs\n",
    "\n",
    "    # The input data should be of the shape\n",
    "    # (num_apply_trajs, self.num_timesteps, self.num_channels)\n",
    "    # The output latent dimensions are of the shape\n",
    "    # (num_apply_trajs, self.latent_dim)\n",
    "    def embed(self, data):\n",
    "        num_apply_trajs, apply_num_channels = data.shape\n",
    "        assert apply_num_channels == self.num_channels, \"channels must match for PCA\"\n",
    "\n",
    "        flattened_trajs = data\n",
    "        mean_vector = self.mean_traj.reshape(1,self.num_channels)\n",
    "        centered_trajs = flattened_trajs - mean_vector\n",
    "        # the dimensions of the embedding matrix are (latent_dim, nt*nc)\n",
    "        latent_vals_mat = centered_trajs @ self.embedding_matrix.T\n",
    "        return(latent_vals_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddb66821",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T05:24:13.460642Z",
     "start_time": "2023-09-29T05:24:13.455851Z"
    }
   },
   "outputs": [],
   "source": [
    "test_time_dtw_vector_timewarper = vtw.DTWVectorTimewarper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abdcf225",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T05:24:14.329445Z",
     "start_time": "2023-09-29T05:24:14.309254Z"
    }
   },
   "outputs": [],
   "source": [
    "def round_trip_loss(ma, traj, i,scale_last_four_dims=1):\n",
    "    assert len(traj.shape) == 2, \"pass in one traj at a time\"\n",
    "    num_ts, channels = traj.shape\n",
    "    assert channels == 2, \"2 channels for handwriting\"\n",
    "    dic = compute_dtw_parameterization(traj)\n",
    "    params = np.concatenate((dic[\"ws_0\"],dic[\"ws_1\"],\n",
    "                                    [dic[\"start_offset_0\"],dic[\"start_offset_1\"],\n",
    "                                    dic[\"g_0\"],dic[\"g_1\"]]\n",
    "                                   )).reshape(1,-1)\n",
    "    params[:,-4:] = scale_last_four_dims * params[:,-4:]\n",
    "    new_parameters = ma.apply(ma.embed(params))\n",
    "    new_parameters[0][-4:] = new_parameters[0][-4:]/scale_last_four_dims\n",
    "    pos = decode_parameter_vector(new_parameters[0])\n",
    "    recon_train = pos.reshape((1,num_ts, channels))\n",
    "    train = traj.reshape((1,num_ts, channels))\n",
    "    \n",
    "    train_dtw_recon, train_dtw_actual = test_time_dtw_vector_timewarper.timewarp_first_and_second(\n",
    "        torch.tensor(recon_train,dtype=torch.float), \n",
    "        torch.tensor(train,dtype=torch.float))\n",
    "    train_aligned_loss = (\n",
    "        nn.functional.mse_loss(train_dtw_recon, train_dtw_actual, reduction=\"sum\").detach().numpy()\n",
    "        / (num_ts))\n",
    "    train_error = np.sum(np.square(recon_train - train))/(num_ts)\n",
    "    \n",
    "    squareresults = (ma.latent_dim, train_aligned_loss, train_error, i)\n",
    "    return squareresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c354477",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T05:30:35.203625Z",
     "start_time": "2023-09-29T05:30:31.426173Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have num_trajs:3750, num_channels:104\n",
      "we have num_trajs:3750, num_channels:104\n",
      "we have num_trajs:3750, num_channels:104\n"
     ]
    }
   ],
   "source": [
    "parameters_vector = np.load(\"parameters_vector_dmps.npy\")\n",
    "for latent_dim in range(5,8):\n",
    "    vals = train_dmp_parameter_model(parameters_vector,latent_dim,scale_last_four_dims=100)\n",
    "    np.savez(f\"dmpmodels/parametric_dmp_{latent_dim}.npz\", dic=vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b8be83b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T05:29:21.783525Z",
     "start_time": "2023-09-29T05:29:21.776904Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvals\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vals' is not defined"
     ]
    }
   ],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81d31ac1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T05:36:07.014720Z",
     "start_time": "2023-09-29T05:30:46.875693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running a dataset\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m---> 21\u001b[0m     square_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mround_trip_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscale_last_four_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     22\u001b[0m square_losses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(square_losses)\n\u001b[1;32m     23\u001b[0m all_results\u001b[38;5;241m.\u001b[39mappend(square_losses)\n",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m, in \u001b[0;36mround_trip_loss\u001b[0;34m(ma, traj, i, scale_last_four_dims)\u001b[0m\n\u001b[1;32m      3\u001b[0m num_ts, channels \u001b[38;5;241m=\u001b[39m traj\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m channels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2 channels for handwriting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m dic \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_dtw_parameterization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m params \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((dic[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mws_0\u001b[39m\u001b[38;5;124m\"\u001b[39m],dic[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mws_1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      7\u001b[0m                                 [dic[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_offset_0\u001b[39m\u001b[38;5;124m\"\u001b[39m],dic[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_offset_1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m                                 dic[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg_0\u001b[39m\u001b[38;5;124m\"\u001b[39m],dic[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg_1\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m      9\u001b[0m                                ))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m params[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m:] \u001b[38;5;241m=\u001b[39m scale_last_four_dims \u001b[38;5;241m*\u001b[39m params[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m:]\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mcompute_dtw_parameterization\u001b[0;34m(traj)\u001b[0m\n\u001b[1;32m     12\u001b[0m g \u001b[38;5;241m=\u001b[39m ydemo[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     13\u001b[0m basisphis \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m(phase_func(ts) \u001b[38;5;241m-\u001b[39m c)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m/\u001b[39m((sigma \u001b[38;5;241m*\u001b[39m c)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cs])\n\u001b[0;32m---> 14\u001b[0m zdemo \u001b[38;5;241m=\u001b[39m \u001b[43mnumeric_integration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mydemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_z\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m ftarget \u001b[38;5;241m=\u001b[39m tau \u001b[38;5;241m*\u001b[39m ydemoprime \u001b[38;5;241m-\u001b[39m zdemo\n\u001b[1;32m     16\u001b[0m ws \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([fit_target_i(i, basisphis, ftarget, xs, yzero, g) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(basisphis))])\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mnumeric_integration\u001b[0;34m(ydemos, ts, tau, g, alpha_z, beta_z)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(ts):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(ts) \u001b[38;5;129;01mand\u001b[39;00m t \u001b[38;5;241m<\u001b[39m ts[i]:\n\u001b[0;32m---> 14\u001b[0m         interp_frac \u001b[38;5;241m=\u001b[39m (\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m/\u001b[39m(ts[i] \u001b[38;5;241m-\u001b[39m ts[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     15\u001b[0m         y \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39minterp_frac) \u001b[38;5;241m*\u001b[39m ydemos[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m interp_frac \u001b[38;5;241m*\u001b[39m ydemos[i]\n\u001b[1;32m     16\u001b[0m         z \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m alpha_z \u001b[38;5;241m*\u001b[39m (beta_z \u001b[38;5;241m*\u001b[39m (g \u001b[38;5;241m-\u001b[39m y) \u001b[38;5;241m-\u001b[39m z)\u001b[38;5;241m/\u001b[39mtau \u001b[38;5;241m*\u001b[39m step_size\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# we trained on augmented data, but we should just apply to regular data\n",
    "\n",
    "DATAFILE=f\"../data/trainTest2DLetterARescaled.npz\"\n",
    "data = np.load(DATAFILE)\n",
    "test = data[\"test\"]\n",
    "train = data[\"train\"]\n",
    "\n",
    "num_trains, num_ts, channels = train.shape\n",
    "num_tests, num_ts, channels = test.shape\n",
    "\n",
    "for latent_dim in range(1,8):\n",
    "    vals = np.load(f\"dmpmodels/parametric_dmp_{latent_dim}.npz\",allow_pickle=True)[\"dic\"].item()\n",
    "    ma = ModelApplier(vals)\n",
    "    all_results = []\n",
    "    for dataset in [train, test]:\n",
    "        print(\"running a dataset\")\n",
    "        square_losses = []\n",
    "        for i in range(50):#len(dataset)):\n",
    "            if i % 10 == 0:\n",
    "                print(i)\n",
    "            square_losses.append(round_trip_loss(ma, dataset[i], i,scale_last_four_dims=100))\n",
    "        square_losses = np.array(square_losses)\n",
    "        all_results.append(square_losses)\n",
    "    np.savez(f\"intermediate_dmp_error_results_{latent_dim}.npz\",train=all_results[0], test=all_results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "588737d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T05:41:53.987927Z",
     "start_time": "2023-09-29T05:41:53.974560Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_results = []\n",
    "for latent_dim in range(1,5):\n",
    "    intermediate_results = np.load(f\"intermediate_dmp_error_results_{latent_dim}.npz\")\n",
    "    valid_inds = intermediate_results[\"train\"][:,0] == latent_dim\n",
    "    #if np.sum(valid_inds) != 125:\n",
    "    #    continue\n",
    "    ld,train_aligned_loss, train_error, checkval = np.mean(intermediate_results[\"train\"][valid_inds,],axis=0)\n",
    "    assert ld == latent_dim, \"check your latent_dim\"\n",
    "    #assert checkval == 62, \"should have indices ranging from 0 to 124\"\n",
    "    \n",
    "    valid_inds = intermediate_results[\"test\"][:,0] == latent_dim\n",
    "    latent_dim,test_aligned_loss, test_error, checkval = np.mean(intermediate_results[\"test\"][valid_inds,],axis=0)\n",
    "    assert ld == latent_dim, \"check your latent_dim\"\n",
    "    #assert checkval == 62, \"should have indices ranging from 0 to 124\"\n",
    "    \n",
    "    final_results.append((latent_dim, np.sqrt(train_aligned_loss),np.sqrt(test_aligned_loss), \n",
    "                       np.sqrt(train_error), np.sqrt(test_error)))\n",
    "final_results = np.array(final_results)\n",
    "np.savez(\"dmp_results.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05868d1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T05:41:54.357247Z",
     "start_time": "2023-09-29T05:41:54.345505Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.84547878, 0.7692925 , 1.82830354, 1.34860321],\n",
       "       [2.        , 0.97586389, 0.82544434, 2.24826507, 1.58517884],\n",
       "       [3.        , 0.94197963, 0.86082859, 2.24365081, 1.61278064],\n",
       "       [4.        , 1.04135748, 0.87249474, 2.31357659, 1.60714652]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14310a84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T03:35:36.398465Z",
     "start_time": "2023-09-29T03:35:35.668801Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parameters_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pos1 \u001b[38;5;241m=\u001b[39m decode_parameter_vector(\u001b[43mparameters_vector\u001b[49m[\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m      2\u001b[0m pos2 \u001b[38;5;241m=\u001b[39m decode_parameter_vector((parameters_vector[\u001b[38;5;241m15\u001b[39m] \u001b[38;5;241m+\u001b[39m parameters_vector[\u001b[38;5;241m10\u001b[39m])\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      3\u001b[0m pos3 \u001b[38;5;241m=\u001b[39m decode_parameter_vector(parameters_vector[\u001b[38;5;241m15\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parameters_vector' is not defined"
     ]
    }
   ],
   "source": [
    "pos1 = decode_parameter_vector(parameters_vector[10])\n",
    "pos2 = decode_parameter_vector((parameters_vector[15] + parameters_vector[10])/2)\n",
    "pos3 = decode_parameter_vector(parameters_vector[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a23f2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T03:35:36.674685Z",
     "start_time": "2023-09-29T03:35:36.400118Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for pos in [pos1, pos2, pos3]:\n",
    "    plt.plot(pos[:,0],pos[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f4657e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
