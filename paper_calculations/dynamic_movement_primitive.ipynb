{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T02:51:45.857097Z",
     "start_time": "2023-09-29T02:51:44.302683Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import timewarp_lib.vector_timewarpers as vtw\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T02:51:45.878711Z",
     "start_time": "2023-09-29T02:51:45.858710Z"
    }
   },
   "outputs": [],
   "source": [
    "tau = 1\n",
    "yzero = 0\n",
    "\n",
    "# arbitrary?\n",
    "alpha_z = 10\n",
    "beta_z = alpha_z/4\n",
    "\n",
    "N = 50\n",
    "sigma = 2/N\n",
    "\n",
    "def phase_func(t):\n",
    "    phasefactor = -np.log(0.01)\n",
    "    return np.exp(-t * phasefactor)\n",
    "\n",
    "cs = phase_func(np.linspace(0,1,N,endpoint=True))\n",
    "\n",
    "fulldat = np.load(\"../data/trainTest2DLetterACache.npz\")\n",
    "fulldat[\"train\"].shape\n",
    "dat = fulldat[\"train\"]\n",
    "numdims = dat.shape[2]\n",
    "numts = dat.shape[1]\n",
    "numtrajs = dat.shape[0]\n",
    "\n",
    "\n",
    "ts = np.linspace(0,1,numts)\n",
    "xs = phase_func(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T02:51:45.884188Z",
     "start_time": "2023-09-29T02:51:45.879840Z"
    }
   },
   "outputs": [],
   "source": [
    "# Yeah, i don't _really_ get it, but an original Ijspeert paper\n",
    "# (and the Learning Parametric Dynamic Movement... paper)\n",
    "# have the forcing term affect velocity, not acceleration\n",
    "def numeric_integration(ydemos, ts, tau, g, alpha_z, beta_z):\n",
    "    step_size = 0.00001\n",
    "    i = 0\n",
    "    t = ts[i]\n",
    "    i += 1\n",
    "    z = 0\n",
    "    zs = []\n",
    "    zs.append(z)\n",
    "    while i < len(ts):\n",
    "        while i < len(ts) and t < ts[i]:\n",
    "            interp_frac = (t - ts[i-1])/(ts[i] - ts[i-1])\n",
    "            y = (1-interp_frac) * ydemos[i-1] + interp_frac * ydemos[i]\n",
    "            z += alpha_z * (beta_z * (g - y) - z)/tau * step_size\n",
    "            t += step_size\n",
    "        zs.append(z)\n",
    "        i += 1\n",
    "    return np.array(zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T02:51:45.888898Z",
     "start_time": "2023-09-29T02:51:45.886067Z"
    }
   },
   "outputs": [],
   "source": [
    "# basisphis, targetfunction, xs are all evaluated at ts\n",
    "def fit_target_i(i, basisphis, targetfunction, xs, yzero, g):\n",
    "    s = xs * (g - yzero)\n",
    "    gamma = np.diag(basisphis[i])\n",
    "    # equation 2.14 from Dynamical Movement Primitives: Learning Attractor Models for Motor Behaviors\n",
    "    numerator = s @ gamma @ targetfunction\n",
    "    denominator = s @ gamma @ s\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T02:51:45.894486Z",
     "start_time": "2023-09-29T02:51:45.890402Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate(fitted_f, ts, g, alpha_z, beta_z):\n",
    "    step_size = 0.00001\n",
    "    i = 0\n",
    "    t = ts[i]\n",
    "    ys = [] # position\n",
    "    zs = [] # velocity\n",
    "    y = 0\n",
    "    z = 0\n",
    "    t = 0\n",
    "    while i < len(ts):\n",
    "        while i < len(ts) and t < ts[i]:\n",
    "            interp_frac = (t - ts[i-1])/(ts[i] - ts[i-1])\n",
    "            f = (1-interp_frac) * fitted_f[i-1] + interp_frac * fitted_f[i]\n",
    "            z += alpha_z * (beta_z * (g - y) - z)/tau * step_size\n",
    "            y += (z + f)/tau * step_size\n",
    "            t += step_size\n",
    "        ys.append(y)\n",
    "        zs.append(z)\n",
    "        i += 1\n",
    "    return (np.array(ys), np.array(zs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute DTW parameterization of each training trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T02:51:45.900894Z",
     "start_time": "2023-09-29T02:51:45.895786Z"
    }
   },
   "outputs": [],
   "source": [
    "# traj should be 2d\n",
    "def compute_dtw_parameterization(traj):\n",
    "    assert len(traj.shape) == 2, \"traj should be 2d...just send in one\"\n",
    "    start_offset_0, start_offset_1 = traj[0]\n",
    "    ydemos = traj-traj[0:1]\n",
    "    result_dictionary = {}\n",
    "    for dim in range(2):\n",
    "        ydemo = ydemos[:,dim]\n",
    "        ydemoprime = (ydemo[2:]-ydemo[:-2])/(ts[1]-ts[0])/2\n",
    "        ydemoprime = np.concatenate(((ydemo[1:2]-ydemo[:1])/(ts[1]-ts[0]),ydemoprime,(ydemo[-1:]-ydemo[-2:-1])/(ts[1]-ts[0])))\n",
    "        yzero = ydemo[0]\n",
    "        g = ydemo[-1]\n",
    "        basisphis = np.array([np.exp(-(phase_func(ts) - c)**2/((sigma * c)**2)) for c in cs])\n",
    "        zdemo = numeric_integration(ydemo, ts, tau, g, alpha_z, beta_z)\n",
    "        ftarget = tau * ydemoprime - zdemo\n",
    "        ws = np.array([fit_target_i(i, basisphis, ftarget, xs, yzero, g) for i in range(len(basisphis))])\n",
    "        result_dictionary[f\"ws_{dim}\"] = ws\n",
    "        result_dictionary[f\"g_{dim}\"] = g\n",
    "    result_dictionary[f\"start_offset_0\"] = start_offset_0\n",
    "    result_dictionary[f\"start_offset_1\"] = start_offset_1\n",
    "    return result_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T03:05:14.653928Z",
     "start_time": "2023-09-29T02:51:45.901966Z"
    }
   },
   "outputs": [],
   "source": [
    "important_results_object = []\n",
    "for i in range(len(dat)):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    result_dictionary = compute_dtw_parameterization(dat[i])\n",
    "    important_results_object.append(result_dictionary)\n",
    "    \n",
    "parameters_vector = np.array([np.concatenate((dic[\"ws_0\"],dic[\"ws_1\"],\n",
    "                                    [dic[\"start_offset_0\"],dic[\"start_offset_1\"],\n",
    "                                    dic[\"g_0\"],dic[\"g_1\"]]\n",
    "                                   ))\n",
    "                     for dic in important_results_object])\n",
    "\n",
    "np.save(\"parameters_vector_dmps.npy\", parameters_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T03:05:14.659670Z",
     "start_time": "2023-09-29T03:05:14.655301Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_parameter_vector(parameter_vector):\n",
    "    ws_0 = parameter_vector[:N]\n",
    "    ws_1 = parameter_vector[N:2*N]\n",
    "    start_offset_0,start_offset_1,g_0,g_1 = parameter_vector[2*N:]\n",
    "    basisphis = np.array([np.exp(-(phase_func(ts) - c)**2/((sigma * c)**2)) for c in cs])\n",
    "    \n",
    "    positions = []\n",
    "    for g,ws,start_offset in [(g_0,ws_0,start_offset_0), (g_1,ws_1,start_offset_1)]:\n",
    "        yzero=0 # centered training data all starts at zero\n",
    "        fitted_f = np.einsum(\"it,i->t\",basisphis,ws)/np.einsum(\"it->t\",basisphis) * xs * (g-yzero)\n",
    "        # ys = position, zs = velocity\n",
    "        ys,zs = simulate(fitted_f,ts,g,alpha_z,beta_z)\n",
    "        positions.append(ys + start_offset)\n",
    "    positions=np.array(positions).T\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T04:53:17.765379Z",
     "start_time": "2023-09-29T04:53:17.761042Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_dmp_parameter_model(training_data, latent_dim,scale_last_four_dims=1):\n",
    "    num_trajs, num_channels = training_data.shape\n",
    "    print(f\"we have num_trajs:{num_trajs}, num_channels:{num_channels}\")\n",
    "    flattened_trajs = np.copy(training_data)\n",
    "    \n",
    "    flattened_trajs[:,-4:] = flattened_trajs[:,-4:] * scale_last_four_dims\n",
    "\n",
    "    # compute and remove the mean trajectory (mean trajectory is of shape (num_timesteps * num_channels))\n",
    "    mean_traj = np.mean(flattened_trajs, axis=0)\n",
    "    centered_trajs = flattened_trajs - mean_traj[np.newaxis,:]\n",
    "\n",
    "    # compute the PCA model using SVD\n",
    "    u, s, vt = np.linalg.svd(centered_trajs)\n",
    "\n",
    "    # keep only the first latent_dim number of dimensions\n",
    "    if len(s) < latent_dim:\n",
    "        raise Exception(f\"You can't build a model of dimension {latent_dim} on a dataset with dimensionality {len(s)}\")\n",
    "\n",
    "    # the singular values written in matrix form\n",
    "    smat = np.diag(s[:latent_dim])\n",
    "    smatinv = np.diag(1./s[:latent_dim])\n",
    "    # the first latent_dim directions of variation\n",
    "    basis_vectors = vt[:latent_dim,:]\n",
    "    # given a trajectory, use this to find its value in the latent space \n",
    "    # note that the dimensions of the embedding matrix are (latent_dim, nt*nc)\n",
    "    embedding_matrix = smatinv @ basis_vectors \n",
    "    # note that we include the s factor here so that our latent space is roughly the unit gaussian ball :brain:\n",
    "    # note further that the dimensions of the reconstruction matrix are also (latent_dim, nt*nc)\n",
    "    reconstruction_matrix = smat @ basis_vectors\n",
    "\n",
    "    return {\n",
    "      \"num_trajs\" : num_trajs, \n",
    "      \"num_channels\" : num_channels,\n",
    "      \"latent_dim\" : latent_dim,\n",
    "      \"mean_traj\" : mean_traj,\n",
    "      \"embedding_matrix\" : embedding_matrix,\n",
    "      \"reconstruction_matrix\" : reconstruction_matrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T04:53:19.195385Z",
     "start_time": "2023-09-29T04:53:19.188685Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a ModelApplier object based purely on a\n",
    "# directory containing model information\n",
    "class ModelApplier(object):\n",
    "    def __init__(self, modeldata):\n",
    "        self.num_trajs = modeldata[\"num_trajs\"]\n",
    "        self.num_channels = modeldata[\"num_channels\"]\n",
    "        self.latent_dim = modeldata[\"latent_dim\"]\n",
    "        self.mean_traj = modeldata[\"mean_traj\"]\n",
    "        self.embedding_matrix = modeldata[\"embedding_matrix\"]\n",
    "        self.reconstruction_matrix = modeldata[\"reconstruction_matrix\"]\n",
    "\n",
    "    # The input data should be of the shape\n",
    "    # (num_apply_trajs, apply_latent_dim)\n",
    "    def apply(self, data):\n",
    "        num_apply_trajs, apply_latent_dim = data.shape\n",
    "        if apply_latent_dim != self.latent_dim:\n",
    "              raise Exception(f\"The number of latent dim coordinates given: {apply_latent_dim} was not the same as the {self.latent_dim} expected by the model\")\n",
    "\n",
    "        # compute the linear combination of basis vectors \n",
    "        traj_offset_vectors = data @ self.reconstruction_matrix\n",
    "        mean_vector = self.mean_traj.reshape(1,self.num_channels)\n",
    "        # add back the mean vector\n",
    "        traj_vectors = traj_offset_vectors + mean_vector\n",
    "        # reshape to the official standard expected shape\n",
    "        # (num_apply_trajs, num_timesteps, num_channels)\n",
    "        result_trajs = traj_vectors.reshape(num_apply_trajs, self.num_channels)\n",
    "        return result_trajs\n",
    "\n",
    "    # The input data should be of the shape\n",
    "    # (num_apply_trajs, self.num_timesteps, self.num_channels)\n",
    "    # The output latent dimensions are of the shape\n",
    "    # (num_apply_trajs, self.latent_dim)\n",
    "    def embed(self, data):\n",
    "        num_apply_trajs, apply_num_channels = data.shape\n",
    "        assert apply_num_channels == self.num_channels, \"channels must match for PCA\"\n",
    "\n",
    "        flattened_trajs = data\n",
    "        mean_vector = self.mean_traj.reshape(1,self.num_channels)\n",
    "        centered_trajs = flattened_trajs - mean_vector\n",
    "        # the dimensions of the embedding matrix are (latent_dim, nt*nc)\n",
    "        latent_vals_mat = centered_trajs @ self.embedding_matrix.T\n",
    "        return(latent_vals_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T04:53:19.345287Z",
     "start_time": "2023-09-29T04:53:19.338120Z"
    }
   },
   "outputs": [],
   "source": [
    "test_time_dtw_vector_timewarper = vtw.DTWVectorTimewarper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T04:53:22.061319Z",
     "start_time": "2023-09-29T04:53:22.050202Z"
    }
   },
   "outputs": [],
   "source": [
    "def round_trip_loss(ma, traj, i,scale_last_four_dims=1):\n",
    "    assert len(traj.shape) == 2, \"pass in one traj at a time\"\n",
    "    num_ts, channels = traj.shape\n",
    "    assert channels == 2, \"2 channels for handwriting\"\n",
    "    dic = compute_dtw_parameterization(traj)\n",
    "    params = np.concatenate((dic[\"ws_0\"],dic[\"ws_1\"],\n",
    "                                    [dic[\"start_offset_0\"],dic[\"start_offset_1\"],\n",
    "                                    dic[\"g_0\"],dic[\"g_1\"]]\n",
    "                                   )).reshape(1,-1)\n",
    "    params[:,-4:] = scale_last_four_dims * params[:,-4:]\n",
    "    new_parameters = ma.apply(ma.embed(params))\n",
    "    new_parameters[0][-4:] = new_parameters[0][-4:]/scale_last_four_dims\n",
    "    pos = decode_parameter_vector(new_parameters[0])\n",
    "    recon_train = pos.reshape((1,num_ts, channels))\n",
    "    train = traj.reshape((1,num_ts, channels))\n",
    "    \n",
    "    train_dtw_recon, train_dtw_actual = test_time_dtw_vector_timewarper.timewarp_first_and_second(\n",
    "        torch.tensor(recon_train,dtype=torch.float), \n",
    "        torch.tensor(train,dtype=torch.float))\n",
    "    train_aligned_loss = (\n",
    "        nn.functional.mse_loss(train_dtw_recon, train_dtw_actual, reduction=\"sum\").detach().numpy()\n",
    "        / (num_ts))\n",
    "    train_error = np.sum(np.square(recon_train - train))/(num_ts)\n",
    "    \n",
    "    squareresults = (ma.latent_dim, train_aligned_loss, train_error, i)\n",
    "    return squareresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T04:53:27.971894Z",
     "start_time": "2023-09-29T04:53:22.939483Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for latent_dim in range(1,5):\n",
    "    vals = train_dmp_parameter_model(parameters_vector,latent_dim,scale_last_four_dims=100)\n",
    "    np.savez(f\"dmpmodels/parametric_dmp_{latent_dim}.npz\", dic=vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T05:05:11.569956Z",
     "start_time": "2023-09-29T04:57:42.227538Z"
    }
   },
   "outputs": [],
   "source": [
    "# we trained on augmented data, but we should just apply to regular data\n",
    "\n",
    "DATAFILE=f\"../data/trainTest2DLetterARescaled.npz\"\n",
    "data = np.load(DATAFILE)\n",
    "test = data[\"test\"]\n",
    "train = data[\"train\"]\n",
    "\n",
    "num_trains, num_ts, channels = train.shape\n",
    "num_tests, num_ts, channels = test.shape\n",
    "\n",
    "for latent_dim in range(1,5):\n",
    "    vals = np.load(f\"dmpmodels/parametric_dmp_{latent_dim}.npz\",allow_pickle=True)[\"dic\"].item()\n",
    "    ma = ModelApplier(vals)\n",
    "    all_results = []\n",
    "    for dataset in [train, test]:\n",
    "        print(\"running a dataset\")\n",
    "        square_losses = []\n",
    "        for i in range(len(dataset)):\n",
    "            if i % 10 == 0:\n",
    "                print(i)\n",
    "            square_losses.append(round_trip_loss(ma, dataset[i], i,scale_last_four_dims=100))\n",
    "        square_losses = np.array(square_losses)\n",
    "        all_results.append(square_losses)\n",
    "    np.savez(f\"intermediate_dmp_error_results_{latent_dim}.npz\",train=all_results[0], test=all_results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T05:05:11.603458Z",
     "start_time": "2023-09-29T05:05:11.571894Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_results = []\n",
    "for latent_dim in range(1,17):\n",
    "    intermediate_results = np.load(f\"intermediate_dmp_error_results_{latent_dim}.npz\")\n",
    "    valid_inds = intermediate_results[\"train\"][:,0] == latent_dim\n",
    "    if np.sum(valid_inds) != 125:\n",
    "        continue\n",
    "    ld,train_aligned_loss, train_error, checkval = np.mean(intermediate_results[\"train\"][valid_inds,],axis=0)\n",
    "    assert ld == latent_dim, \"check your latent_dim\"\n",
    "    assert checkval == 62, \"should have indices ranging from 0 to 124\"\n",
    "    \n",
    "    valid_inds = intermediate_results[\"test\"][:,0] == latent_dim\n",
    "    latent_dim,test_aligned_loss, test_error, checkval = np.mean(intermediate_results[\"test\"][valid_inds,],axis=0)\n",
    "    assert ld == latent_dim, \"check your latent_dim\"\n",
    "    assert checkval == 62, \"should have indices ranging from 0 to 124\"\n",
    "    \n",
    "    final_results.append((latent_dim, np.sqrt(train_aligned_loss),np.sqrt(test_aligned_loss), \n",
    "                       np.sqrt(train_error), np.sqrt(test_error)))\n",
    "final_results = np.array(final_results)\n",
    "np.savez(\"dmp_results.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T05:05:11.609136Z",
     "start_time": "2023-09-29T05:05:11.604799Z"
    }
   },
   "outputs": [],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T03:35:36.398465Z",
     "start_time": "2023-09-29T03:35:35.668801Z"
    }
   },
   "outputs": [],
   "source": [
    "pos1 = decode_parameter_vector(parameters_vector[10])\n",
    "pos2 = decode_parameter_vector((parameters_vector[15] + parameters_vector[10])/2)\n",
    "pos3 = decode_parameter_vector(parameters_vector[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T03:35:36.674685Z",
     "start_time": "2023-09-29T03:35:36.400118Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for pos in [pos1, pos2, pos3]:\n",
    "    plt.plot(pos[:,0],pos[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
