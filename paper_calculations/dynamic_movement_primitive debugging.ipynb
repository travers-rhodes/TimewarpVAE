{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6388b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import timewarp_lib.vector_timewarpers as vtw\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafeae14",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffd8c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldat = np.load(\"../data/trainTest2DLetterACache.npz\")\n",
    "print(fulldat[\"train\"].shape)\n",
    "dat = fulldat[\"train\"]\n",
    "numdims = dat.shape[2]\n",
    "numts = dat.shape[1]\n",
    "numtrajs = dat.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53677456",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "A DMP is a combination of a simple attractor-to-goal model and a forcing term.\n",
    "\n",
    "The forcing term is a function of phase `x` which (without external disturbances) is a direct function of time `t`.\n",
    "DMP models from the original Ijspeert paper\n",
    "(and the Learning Parametric Dynamic Movement... paper)\n",
    "have the forcing term affect velocity, not acceleration.\n",
    "\n",
    "The forcing term is a linear combination of kernel functions.\n",
    "The linear combination values is computed using locally weighted regression on \n",
    "the difference between the velocities that would exist at phase `x`\n",
    "if we just had the attractor-to-goal and no forcing term and no external velocities\n",
    "and the trajectory's actual velocity at phase `x`.\n",
    "Note that this does not find the minimum squared positional error optimal weights.\n",
    "Both because it is fitting velocities, not positions,\n",
    "and because it is using a local fitting procedure, not finding the optimal combination\n",
    "to reduce error across the whole trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20be7b1",
   "metadata": {},
   "source": [
    "# Phase vs Time\n",
    "\n",
    "We don't use any of the flexibility that this additional complication allows,\n",
    "but for consistency with Ijspeert 2002,\n",
    "instead of functions of `t` for kernel functions, we use kernel functions that are \n",
    "a function of `x` according to the equations below.\n",
    "This additional complication is only useful if 1) you have external perturbations during motion\n",
    "and 2) you add some type of delaying/forcing function to how the phase evolves over time.\n",
    "$$ \\tau \\dot v = \\alpha_v (\\beta_v (g-x) - v) $$\n",
    "$$ \\tau \\dot x = v $$\n",
    "Since we don't use any of the additional flexibility that this equation allows, this is exactly the same as saying that\n",
    "$$ x = a e^{-\\phi_1 t} + b e^{-\\phi_2 t} $$\n",
    "Where the $\\phi$s are $\\frac 1 {2\\tau} (-\\alpha_v \\pm \\sqrt{\\alpha_v^2 - 4 \\alpha_v \\beta_v})$\n",
    "but since $\\alpha_v$ and $\\beta_v$ are chosen arbitrarily, we can instead just arbitrarily set \n",
    "$x = e^{-\\phi t}$ with $\\phi = \\log(100)$, making  $x_0 = 1$ and $\\dot x_0 = -\\phi$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464550d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1\n",
    "def phase_func(t):\n",
    "    phasefactor = np.log(100)\n",
    "    return np.exp(-t * phasefactor)\n",
    "# ts is the (scaled) timestamp\n",
    "ts = np.linspace(0,1,numts)\n",
    "# xs is the associated ``phase''\n",
    "xs = phase_func(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d71617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts,xs)\n",
    "plt.xlabel(\"Time t\")\n",
    "plt.ylabel(\"Phase Value x\")\n",
    "plt.title(\"Phase is function of time\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef933473",
   "metadata": {},
   "source": [
    "## Basis Functions\n",
    "The forcing function is a linear combination of basis functions.\n",
    "The `N` different basis functions are computed as Gaussians over the phase, with equally spaced centers\n",
    "``cs`` and standard deviation `sigma` that scales with the center of the basis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 60\n",
    "cs = phase_func(np.linspace(0,1,N,endpoint=True))\n",
    "sigma = 2/N\n",
    "\n",
    "# basisphis is N x numts\n",
    "# Note that instead of using a constant `sigma`, we use `sigma * c`.\n",
    "# This makes each basis function nicely translationally-symmetric with others\n",
    "# as a funciton of time. (compare, eg, to Ijspeert 2002's Figure 1, which is grossly)\n",
    "basisphis = np.array([np.exp(-(xs - c)**2/((sigma * c)**2)) for c in cs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647f0e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts,basisphis.T)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Unitless\")\n",
    "plt.title(\"Basis Functions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ca741",
   "metadata": {},
   "source": [
    "# Estimating forcing function from data\n",
    "\n",
    "Ijspeert 2002 explains:\n",
    "For learning from a given sample trajectory,\n",
    "characterized by a trajectory y demo (t), $\\dot y$ demo (t) and duration T , a supervised learning problem can be formulated with the target trajectory f target = τ $\\dot y$ demo − z demo\n",
    "for Eq.1 (right), where z demo is obtained by integrating Eq.1 (left) with y demo instead of y.\n",
    "\n",
    "The equations (Eq. 1) of Ijspeert 2002 are:\n",
    "$$ \\tau \\dot z = \\alpha_z (\\beta_z(g-y)-z) $$\n",
    "$$ \\tau \\dot y = z + f $$\n",
    "\n",
    "And the integration to solve for $z(t)$ is (using $s$ instead of $t$ for evaluation to avoid\n",
    "using same symbol $t$ for $dt$ and for limit of integration):\n",
    "$$ z(s) = \\frac 1 \\tau \\int_0^s \\alpha_z (\\beta_z (g-y_{demo}) - z) dt $$\n",
    "\n",
    "In this way, we're estimating the forcing function that \"must have existed\" during the training trajectory.\n",
    "\n",
    "The $z(t)$ we're calculating is hard to think about, but it can be thought of as \"the component of the velocity due to the goal attractor\". And the total velocity of the trajectory is that value $z$ plus the forcing term $f$ (times the time scaling term $\\tau$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f690e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we choose these according to the suggested values in Ijspeert 2013 (didn't see suggestions in Ijspeert 2002)\n",
    "alpha_z = 25\n",
    "beta_z = alpha_z/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef41c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform numeric integration on the training data to estimate\n",
    "# the component of the velocity of the trajectory that is due to the \"point-attractor\"\n",
    "# component of the trajectory model.\n",
    "# The rest of the velocity of the trajectory will be due to the \"forcing term\" f\n",
    "def numeric_integration(ydemos, ts, tau, g, alpha_z, beta_z):\n",
    "    # We perform numeric integration using small rectangles\n",
    "    # with width dt = step_size\n",
    "    # and height equal the value of 1/tau \\alpha_z (\\beta_z (g-y_{demo}) - z)\n",
    "    # evaluated at the left of the rectangle.\n",
    "    step_size = 0.00001\n",
    "    # start integrating at the first ts\n",
    "    t = ts[0]\n",
    "    # Since our y_demo is only known at certain places,\n",
    "    # keep track of the index i\n",
    "    # of the smallest ts that is larger than our current value of t\n",
    "    # when we want to compute y_delta at some t between ts[i-1] and ts[i]\n",
    "    # we do linear interpolation\n",
    "    i = 1\n",
    "    # keep track of our cumulative sum (which numerically estimates the integral)\n",
    "    z = 0\n",
    "    # keep track of the integral as it progresses.\n",
    "    # append a 0 to start, to avoid the fencepost problem (otherwise we have one fewer zs than ts)\n",
    "    zs = []\n",
    "    zs.append(z)\n",
    "    # compute the integral up to each of the t values in ts, appending the result to zs each time\n",
    "    while i < len(ts):\n",
    "        # keep on adding dt to t and summing the rectangle value until we hit the next value in ts\n",
    "        while i < len(ts) and t < ts[i]:\n",
    "            # we use linear interpolation to compute the ydemo value at the intermediate t value\n",
    "            interp_frac = (t - ts[i-1])/(ts[i] - ts[i-1])\n",
    "            y = (1-interp_frac) * ydemos[i-1] + interp_frac * ydemos[i]\n",
    "            # compute the area of the rectangle and add it to our cumulative integration.\n",
    "            # note that z appears on the left and right of this expression\n",
    "            z = z + (alpha_z * (beta_z * (g - y) - z)/tau * step_size)\n",
    "            t += step_size\n",
    "        zs.append(z)\n",
    "        i += 1\n",
    "    return np.array(zs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9453c4d",
   "metadata": {},
   "source": [
    "# Example Fitting Single Dim of Single Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096657e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zdemos is (what) the velocity of the trajectory (would be, if just the attractor model)\n",
    "zdemos = []\n",
    "# ftargets is the imputed forcing function\n",
    "ftargets = []\n",
    "# both those are arrays of arrays, with indices according to\n",
    "# forcing velocity = ftargets[trajectory index][dimension]\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    ftargets.append([])\n",
    "    zdemos.append([])\n",
    "    for dim in range(2):\n",
    "        print(\"dim\",dim)        \n",
    "        ydemo = dat[i,:,dim]\n",
    "        plt.plot(ts,ydemo)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Position\")\n",
    "        ydemoprime = (ydemo[2:]-ydemo[:-2])/(ts[1]-ts[0])/2\n",
    "        ydemoprime = np.concatenate(((ydemo[1:2]-ydemo[:1])/(ts[1]-ts[0]),ydemoprime,(ydemo[-1:]-ydemo[-2:-1])/(ts[1]-ts[0])))\n",
    "\n",
    "        yzero = ydemo[0]\n",
    "        g = ydemo[-1]\n",
    "        \n",
    "        zdemo = numeric_integration(ydemo, ts, tau, g, alpha_z, beta_z)\n",
    "        ftarget = tau * ydemoprime - zdemo\n",
    "        ftargets[-1].append(ftarget)\n",
    "        zdemos[-1].append(zdemo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468557a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# indices are [trajectory_index][dimension]\n",
    "plt.scatter(ts,zdemos[0][0],label=\"z\")\n",
    "plt.scatter(ts,-ftargets[0][0],label=\"f\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Velocity\")\n",
    "plt.title(\"Velocity from point attractor (z) and from forcing function (f)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044858d8",
   "metadata": {},
   "source": [
    "###  Fit the forcing function\n",
    "Use locally-weighted regression to fit the forcing function.\n",
    "Note that this is a very particular way of fitting $f$ using the basis functions.\n",
    "\n",
    "In Ijspeert 2002, the locally weighted regression fits $f$ as a scalar coefficient $w$ times our canonical velocity $v$ (using a local weighting factor $\\psi_i$).\n",
    "\n",
    "However, since we have a very, very simple canonical model, we the canonical velocity $v = \\tau \\dot x$, which is proportional to the phase $x$. So it doesn't really matter whether we fit to $v$ or $x$. The coefficients will only differ by a constant factor.\n",
    "Likewise, the scaling by `(g-yzero)` only affects the relative scaling of the coefficients between different trajectories. For a single trajectory that will just scale all the coefficients by a constant.\n",
    "So, it seems fine to incorporate that from Ijspeert 2013 as well.\n",
    "\n",
    "Thus, we fit `f(t)` as a multiple of `x(t)(g-yzero)` around the region where $\\psi_i$ is active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd9ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the weighting kernel $\\psi_i$ corresponding to the `i`'th index of `basisphis`\n",
    "# Model the `targetfunction` $f$ as a linear multiple of `xs*(g-yzero)`\n",
    "# to minimize $\\psi_i * (f - xs*(g-yzero))^2$\n",
    "# Note that basisphis[i], targetfunction, xs are all vectors where each index\n",
    "# corresponds to a different t in ts.\n",
    "def fit_target_i(i, basisphis, targetfunction, xs, yzero, g):\n",
    "    # scale the xs by the scaling factor associated with the training trajectory\n",
    "    s = xs * (g - yzero)\n",
    "    # create the diagonal matrix associated with the basis function\n",
    "    gamma = np.diag(basisphis[i])\n",
    "    # equation 2.14 from Dynamical Movement Primitives: Learning Attractor Models for Motor Behaviors\n",
    "    numerator = s @ gamma @ targetfunction\n",
    "    denominator = s @ gamma @ s\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246940bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wss = []\n",
    "fitted_fs = []\n",
    "\n",
    "# for a training trajectory\n",
    "for i in range(1):\n",
    "    # compute, for each dimension:\n",
    "    # the weights of each basis functions\n",
    "    # and the resulting basis function.\n",
    "    # wss is indexable as [trajectoryId][dimension][basisindex]\n",
    "    # fitted_fs is indexable as [trajectoryId][dimension][ts]\n",
    "    wss.append([])\n",
    "    fitted_fs.append([])\n",
    "    for dim in range(2):\n",
    "        # take in one dimension of one trajectory\n",
    "        ydemo = dat[i,:,dim]\n",
    "        # get its start and end for scaling\n",
    "        yzero = ydemo[0]\n",
    "        g = ydemo[-1]\n",
    "        # compute the weight for each basis function\n",
    "        # that best matches the already-computed ftargets\n",
    "        ws = np.array([fit_target_i(basisIndex, basisphis, ftargets[i][dim], xs, yzero, g) for basisIndex in range(len(basisphis))])\n",
    "        # compute the reconstructed ftargets that those weights generate (scaling back up again using g-yzero)\n",
    "        fitted_f = np.einsum(\"it,i->t\",basisphis,ws)/np.einsum(\"it->t\",basisphis) * xs * (g-yzero)\n",
    "        # save the weights and the fitted model\n",
    "        wss[-1].append(ws)\n",
    "        fitted_fs[-1].append(fitted_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1728e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices are [trajectory_index][dimension]\n",
    "plt.scatter(ts,ftargets[0][0],label=\"f\")\n",
    "plt.scatter(ts,fitted_fs[0][0],label=\"fitted_f\",c=\"C1\")\n",
    "# get the trajectory dimension start and end for scaling\n",
    "ydemo = dat[0,:,0]\n",
    "yzero = ydemo[0]\n",
    "g = ydemo[-1]\n",
    "for basisIndex in range(len(wss[0][0])):\n",
    "    plt.plot(ts,basisphis[basisIndex] * wss[0][0][basisIndex] * xs * (g-yzero)/np.einsum(\"it->t\",basisphis))\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Velocity\")\n",
    "plt.title(\"Forcing function and fit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cb4322",
   "metadata": {},
   "source": [
    "## Reconstruct trajectory\n",
    "From the weights we reconstructed the forcing function `fitted_f`.\n",
    "We can then integrate and simulate forward to see what the point-attractor and forcing function together would cause the\n",
    "trajectory to actually be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bfd633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(fitted_f, ts, g, alpha_z, beta_z, yzero):\n",
    "    step_size = 0.00001\n",
    "    i = 0\n",
    "    t = ts[i]\n",
    "    ys = [] # position\n",
    "    zs = [] # velocity\n",
    "    y = yzero\n",
    "    z = 0\n",
    "    t = 0\n",
    "    while i < len(ts):\n",
    "        while i < len(ts) and t < ts[i]:\n",
    "            interp_frac = (t - ts[i-1])/(ts[i] - ts[i-1])\n",
    "            f = (1-interp_frac) * fitted_f[i-1] + interp_frac * fitted_f[i]\n",
    "            z += alpha_z * (beta_z * (g - y) - z)/tau * step_size\n",
    "            y += (z + f)/tau * step_size\n",
    "            t += step_size\n",
    "        ys.append(y)\n",
    "        zs.append(z)\n",
    "        i += 1\n",
    "    return (np.array(ys), np.array(zs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda55166",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in range(2):\n",
    "    # get the trajectory dimension start and end for scaling\n",
    "    ydemo = dat[0,:,dim]\n",
    "    yzero = ydemo[0]\n",
    "    g = ydemo[-1]\n",
    "    recon_ys,recon_zs = simulate(fitted_fs[0][dim],ts,g,alpha_z,beta_z, yzero)\n",
    "    plt.plot(ts, ydemo,   label=\"actual\")\n",
    "    plt.plot(ts, recon_ys,label=\"recon\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Position\")\n",
    "    plt.title(f\"Reconstruction Dim {dim}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440f8fe8",
   "metadata": {},
   "source": [
    "## Single Function to Compute DTW parameterization of training trajectory\n",
    "Write a single function to compute the DTW parameters computed in the steps above\n",
    "(and confirm you get the same answer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6f557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traj should be 2d\n",
    "def compute_dtw_parameterization(traj, basisphis, ts, xs, tau, alpha_z, beta_z):\n",
    "    assert len(traj.shape) == 2, \"traj should be 2d...just send in one trajectory\"\n",
    "    # reset the trajectory to start at (0,0)\n",
    "    # and save the actual starting position to start_offset variables\n",
    "    start_offset_0, start_offset_1 = traj[0]\n",
    "    ydemos = traj-traj[0:1]\n",
    "    result_dictionary = {}\n",
    "    for dim in range(2):\n",
    "        ydemo = ydemos[:,dim]\n",
    "        ydemoprime = (ydemo[2:]-ydemo[:-2])/(ts[1]-ts[0])/2\n",
    "        ydemoprime = np.concatenate(((ydemo[1:2]-ydemo[:1])/(ts[1]-ts[0]),ydemoprime,(ydemo[-1:]-ydemo[-2:-1])/(ts[1]-ts[0])))\n",
    "        yzero = ydemo[0]\n",
    "        assert yzero == 0 , \"We shifted training trajectory to start at 0\"\n",
    "        g = ydemo[-1]\n",
    "        zdemo = numeric_integration(ydemo, ts, tau, g, alpha_z, beta_z)\n",
    "        ftarget = tau * ydemoprime - zdemo\n",
    "        ws = np.array([fit_target_i(basisIndex, basisphis, ftarget, xs, yzero, g) for basisIndex in range(len(basisphis))])\n",
    "        result_dictionary[f\"ws_{dim}\"] = ws\n",
    "        result_dictionary[f\"g_{dim}\"] = g\n",
    "    result_dictionary[f\"start_offset_0\"] = start_offset_0\n",
    "    result_dictionary[f\"start_offset_1\"] = start_offset_1\n",
    "    return result_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442cc034",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = compute_dtw_parameterization(dat[0,:,:], basisphis, ts, xs, tau, alpha_z, beta_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12871eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to make sure that our single-function-solution\n",
    "# gets the same answer as our step-by-step approach above\n",
    "for dim in range(2):\n",
    "    np.testing.assert_almost_equal(trained_model[f\"ws_{dim}\"],wss[0][dim])\n",
    "    np.testing.assert_almost_equal(trained_model[f\"g_{dim}\"],dat[0,-1,dim] - dat[0,0,dim])\n",
    "    np.testing.assert_almost_equal(trained_model[f\"start_offset_{dim}\"],dat[0,0,dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762c3063",
   "metadata": {},
   "source": [
    "## Single function to reconstruct trajectory from parameter vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dic_to_parameter_vector(dic):\n",
    "    params = np.concatenate((dic[\"ws_0\"],dic[\"ws_1\"],\n",
    "                                    [dic[\"start_offset_0\"],dic[\"start_offset_1\"],\n",
    "                                    dic[\"g_0\"],dic[\"g_1\"]]\n",
    "                                   )).reshape(1,-1)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258dc12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_parameter_vector(parameter_vector, basisphis, xs, N):\n",
    "    ws_0 = parameter_vector[:N]\n",
    "    ws_1 = parameter_vector[N:2*N]\n",
    "    start_offset_0,start_offset_1,g_0,g_1 = parameter_vector[2*N:]\n",
    "    \n",
    "    positions = []\n",
    "    for g, ws, start_offset in [(g_0,ws_0,start_offset_0), (g_1,ws_1,start_offset_1)]:\n",
    "        yzero=0 # centered training data all starts at zero\n",
    "        fitted_f = np.einsum(\"it,i->t\",basisphis,ws)/np.einsum(\"it->t\",basisphis) * xs * (g-yzero)\n",
    "        # ys = position, zs = velocity\n",
    "        ys,zs = simulate(fitted_f,ts,g,alpha_z,beta_z,yzero)\n",
    "        positions.append(ys + start_offset)\n",
    "    positions=np.array(positions).T\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b70e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_vect = convert_dic_to_parameter_vector(trained_model)[0]\n",
    "positions = decode_parameter_vector(param_vect, basisphis, xs, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc08334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to make sure that our single-function-solution\n",
    "# gets the same answer as our step-by-step approach above\n",
    "for dim in range(2):\n",
    "    # get the trajectory dimension start and end for scaling\n",
    "    ydemo = dat[0,:,dim]\n",
    "    yzero = ydemo[0]\n",
    "    g = ydemo[-1]\n",
    "    recon_ys,recon_zs = simulate(fitted_fs[0][dim],ts,g,alpha_z,beta_z, yzero)\n",
    "    np.testing.assert_almost_equal(positions[:,dim],recon_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c516c1",
   "metadata": {},
   "source": [
    "# Train a linear PCA model on the dmp parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a790d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dmp_parameter_model(training_data, latent_dim,cs, scale_last_four_dims):\n",
    "    num_trajs, num_channels = training_data.shape\n",
    "    print(f\"we have num_trajs:{num_trajs}, num_channels:{num_channels}\")\n",
    "    scaled_params = np.copy(training_data)\n",
    "    \n",
    "    scaled_params[:,-4:] = scaled_params[:,-4:] * scale_last_four_dims\n",
    "    #\n",
    "    # Right. Ijspeert uses the phase (instead of a constant) as the factor\n",
    "    # used to model the forcing function. So parameters near the beginning of the\n",
    "    # trajectory will naturally have lower values, since they're modeled by something larger.\n",
    "    # multiply by an estimate of that scaling factor\n",
    "    #\n",
    "    N = len(cs)\n",
    "    scaled_params[:,:N] = scaled_params[:,:N] * cs\n",
    "    scaled_params[:,N:2*N] = scaled_params[:,N:2*N] * cs\n",
    "    \n",
    "    \n",
    "\n",
    "    # compute and remove the mean parameter values (mean parameter values is of shape (num_channels))\n",
    "    mean_params = np.mean(scaled_params, axis=0)\n",
    "    assert len(mean_params) == N*2 + 4, \"mean parameter should be valid parameterization\"\n",
    "    centered_params = scaled_params - mean_params[np.newaxis,:]\n",
    "\n",
    "    # compute the PCA model using SVD\n",
    "    u, s, vt = np.linalg.svd(centered_params)\n",
    "    \n",
    "\n",
    "    # keep only the first latent_dim number of dimensions\n",
    "    if len(s) < latent_dim:\n",
    "        raise Exception(f\"You can't build a model of dimension {latent_dim} on a dataset with dimensionality {len(s)}\")\n",
    "\n",
    "    # the singular values written in matrix form\n",
    "    smat = np.diag(s[:latent_dim])\n",
    "    smatinv = np.diag(1./s[:latent_dim])\n",
    "    # the first latent_dim directions of variation\n",
    "    basis_vectors = vt[:latent_dim,:]\n",
    "    # given a trajectory, use this to find its value in the latent space \n",
    "    # note that the dimensions of the embedding matrix are (latent_dim, nt*nc)\n",
    "    embedding_matrix = smatinv @ basis_vectors \n",
    "    # note that we include the s factor here so that our latent space is roughly the unit gaussian ball :brain:\n",
    "    # note further that the dimensions of the reconstruction matrix are also (latent_dim, nt*nc)\n",
    "    reconstruction_matrix = smat @ basis_vectors\n",
    "    \n",
    "    # store the basis_vectors too,\n",
    "    # so that we can avoid numerical imprecision from smatinv * smat\n",
    "\n",
    "    return {\n",
    "      \"num_trajs\" : num_trajs,\n",
    "      \"num_channels\" : num_channels,\n",
    "      \"latent_dim\" : latent_dim,\n",
    "      \"mean_params\" : mean_params,\n",
    "      \"embedding_matrix\" : embedding_matrix,\n",
    "      \"reconstruction_matrix\" : reconstruction_matrix,\n",
    "      \"basis_vectors\" : basis_vectors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f765e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ModelApplier object based purely on a\n",
    "# directory containing model information\n",
    "class ModelApplier(object):\n",
    "    def __init__(self, modeldata):\n",
    "        self.num_trajs = modeldata[\"num_trajs\"]\n",
    "        self.num_channels = modeldata[\"num_channels\"]\n",
    "        self.latent_dim = modeldata[\"latent_dim\"]\n",
    "        self.mean_params = modeldata[\"mean_params\"]\n",
    "        self.embedding_matrix = modeldata[\"embedding_matrix\"]\n",
    "        self.reconstruction_matrix = modeldata[\"reconstruction_matrix\"]\n",
    "        self.basis_vectors = modeldata[\"basis_vectors\"]\n",
    "\n",
    "    # The input data should be of the shape\n",
    "    # (num_apply_trajs, apply_latent_dim)\n",
    "    def recon(self, data):\n",
    "        num_apply_trajs, apply_latent_dim = data.shape\n",
    "        if apply_latent_dim != self.latent_dim:\n",
    "              raise Exception(f\"The number of latent dim coordinates given: {apply_latent_dim} was not the same as the {self.latent_dim} expected by the model\")\n",
    "\n",
    "        # compute the linear combination of basis vectors \n",
    "        traj_offset_vectors = data @ self.reconstruction_matrix\n",
    "        mean_vector = self.mean_params.reshape(1,self.num_channels)\n",
    "        # add back the mean vector\n",
    "        traj_vectors = traj_offset_vectors + mean_vector\n",
    "        # reshape to the official standard expected shape\n",
    "        # (num_apply_trajs, num_timesteps, num_channels)\n",
    "        result_trajs = traj_vectors.reshape(num_apply_trajs, self.num_channels)\n",
    "        return result_trajs\n",
    "\n",
    "    # The input data should be of the shape\n",
    "    # (num_apply_trajs, self.num_channels)\n",
    "    # The output latent dimensions are of the shape\n",
    "    # (num_apply_trajs, self.latent_dim)\n",
    "    def embed(self, data):\n",
    "        num_apply_trajs, apply_num_channels = data.shape\n",
    "        assert apply_num_channels == self.num_channels, \"channels must match for PCA\"\n",
    "\n",
    "        flattened_trajs = data\n",
    "        mean_vector = self.mean_params.reshape(1,self.num_channels)\n",
    "        centered_trajs = flattened_trajs - mean_vector\n",
    "        # the dimensions of the embedding matrix are (latent_dim, nc)\n",
    "        latent_vals_mat = centered_trajs @ self.embedding_matrix.T\n",
    "        return(latent_vals_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635fea13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da720056",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_param = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ea2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e572e9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# trivial check that a 1D model on 2 parameters gives perfect reconstructions of that model\n",
    "two_dicts = [compute_dtw_parameterization(dat[i,:,:], basisphis, ts, xs, tau, alpha_z, beta_z) for i in range(2)]\n",
    "two_vectors = np.concatenate([convert_dic_to_parameter_vector(two_dicts[i]) for i in range(2)],axis=0)\n",
    "       \n",
    "# compute dtw parameters for two trajectories\n",
    "\n",
    "perf_model_params = train_dmp_parameter_model(two_vectors, 1, cs, scale_param)\n",
    "perf_model = ModelApplier(perf_model_params)\n",
    "\n",
    "np.testing.assert_almost_equal(np.sum(np.square(perf_model.basis_vectors[0])),1)\n",
    "\n",
    "scaled_two_vectors = np.copy(two_vectors)\n",
    "scaled_two_vectors[:,-4:] *= scale_param\n",
    "scaled_two_vectors[:,:N] = scaled_two_vectors[:,:N] * cs\n",
    "scaled_two_vectors[:,N:2*N] = scaled_two_vectors[:,N:2*N] * cs\n",
    "\n",
    "np.testing.assert_almost_equal(perf_model.recon(perf_model.embed(scaled_two_vectors)), scaled_two_vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fcc638",
   "metadata": {},
   "source": [
    "## Given a PCA model on training data, compute round-trip test error\n",
    "Round-trip test error computed with and without DTW temporal alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb66821",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_time_dtw_vector_timewarper = vtw.DTWVectorTimewarper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdcf225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_trip_loss(ma, traj, basisphis, logging_index, ts, xs, tau, alpha_z, beta_z, cs, scale_last_four_dims):\n",
    "    N = len(basisphis)\n",
    "    assert len(traj.shape) == 2, \"pass in one traj at a time\"\n",
    "    num_ts, channels = traj.shape\n",
    "    assert channels == 2, \"2 channels for handwriting\"\n",
    "    dic = compute_dtw_parameterization(traj, basisphis, ts, xs, tau, alpha_z, beta_z)\n",
    "    params = np.concatenate((dic[\"ws_0\"],dic[\"ws_1\"],\n",
    "                                    [dic[\"start_offset_0\"],dic[\"start_offset_1\"],\n",
    "                                    dic[\"g_0\"],dic[\"g_1\"]]\n",
    "                                   )).reshape(1,-1)\n",
    "    params_copy = np.copy(params)\n",
    "    params_copy[:,-4:] = scale_last_four_dims * params_copy[:,-4:]\n",
    "    params_copy[:,:N] = params_copy[:,:N] * cs\n",
    "    params_copy[:,N:2*N] = params_copy[:,N:2*N] * cs\n",
    "    \n",
    "    new_parameters = ma.recon(ma.embed(params_copy))\n",
    "    recon_scaled = np.copy(new_parameters)\n",
    "    new_parameters[0][-4:] = new_parameters[0][-4:]/scale_last_four_dims\n",
    "    new_parameters[:,:N] = new_parameters[:,:N] / cs\n",
    "    new_parameters[:,N:2*N] = new_parameters[:,N:2*N] / cs\n",
    "    pos = decode_parameter_vector(new_parameters[0], basisphis, xs, N)\n",
    "    recon_train = pos.reshape((1,num_ts, channels))\n",
    "    train = traj.reshape((1,num_ts, channels))\n",
    "    #plt.plot(train[0,:,0],train[0,:,1],label=\"train\")\n",
    "    #plt.plot(recon_train[0,:,0],recon_train[0,:,1],label=\"recon\")\n",
    "    #plt.show()\n",
    "    #\n",
    "    ##plt.plot(params[0])\n",
    "    #plt.plot(params_copy[0])\n",
    "    #plt.plot(recon_scaled[0])\n",
    "    #plt.show()\n",
    "    \n",
    "    train_dtw_recon, train_dtw_actual = test_time_dtw_vector_timewarper.timewarp_first_and_second(\n",
    "        torch.tensor(recon_train,dtype=torch.float), \n",
    "        torch.tensor(train,dtype=torch.float))\n",
    "    train_aligned_loss = (\n",
    "        nn.functional.mse_loss(train_dtw_recon, train_dtw_actual, reduction=\"sum\").detach().numpy()\n",
    "        / (num_ts))\n",
    "    train_error = np.sum(np.square(recon_train - train))/(num_ts)\n",
    "    \n",
    "    params_error = np.sqrt(np.sum(np.square(params_copy - recon_scaled)))\n",
    "    squareresults = (ma.latent_dim, train_aligned_loss, train_error, logging_index, params_error)\n",
    "    return squareresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't have a perfect round-trip loss because\n",
    "# the dmp model applied by itself still has some error.\n",
    "\n",
    "#round-trip error for a perfect linear model that exactly reconstructs dmp parameters\n",
    "trajid = 0\n",
    "(_,_,train_err,_,_) = round_trip_loss(perf_model, dat[trajid],basisphis,trajid, ts, xs, tau, alpha_z, beta_z, cs, scale_param)\n",
    "\n",
    "# error for the dmp model\n",
    "combined_squared_error = 0\n",
    "for dim in range(2):\n",
    "    # get the trajectory dimension start and end for scaling\n",
    "    ydemo = dat[0,:,dim]\n",
    "    yzero = ydemo[0]\n",
    "    g = ydemo[-1]\n",
    "    recon_ys,recon_zs = simulate(fitted_fs[0][dim],ts,g,alpha_z,beta_z, yzero)\n",
    "    combined_squared_error += np.mean((recon_ys - ydemo)**2)\n",
    "    \n",
    "# they tie out\n",
    "np.testing.assert_almost_equal(train_err, combined_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7a14e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_results_object = []\n",
    "for i in range(len(dat)):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    result_dictionary = compute_dtw_parameterization(dat[i], basisphis, ts, xs, tau, alpha_z, beta_z)\n",
    "    important_results_object.append(result_dictionary)\n",
    "    \n",
    "parameters_vector = np.array([np.concatenate((dic[\"ws_0\"],dic[\"ws_1\"],\n",
    "                                    [dic[\"start_offset_0\"],dic[\"start_offset_1\"],\n",
    "                                    dic[\"g_0\"],dic[\"g_1\"]]\n",
    "                                   ))\n",
    "                     for dic in important_results_object])\n",
    "\n",
    "np.save(\"dmpmodels/parameters_vector_dmps.npy\", parameters_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c354477",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters_vector = np.load(\"dmpmodels/parameters_vector_dmps.npy\")\n",
    "for latent_dim in range(1,17):\n",
    "    vals = train_dmp_parameter_model(parameters_vector,latent_dim,cs, scale_last_four_dims=scale_param)\n",
    "    np.savez(f\"dmpmodels/parametric_dmp_{latent_dim}.npz\", dic=vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039fdb9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATAFILE=f\"../data/trainTest2DLetterARescaled.npz\"\n",
    "data = np.load(DATAFILE)\n",
    "test = data[\"test\"]\n",
    "train = data[\"train\"]\n",
    "num_trains, num_ts, channels = train.shape\n",
    "num_tests, num_ts, channels = test.shape\n",
    "\n",
    "for latent_dim in range(1,17):\n",
    "    print(latent_dim)\n",
    "    vals = np.load(f\"dmpmodels/parametric_dmp_{latent_dim}.npz\",allow_pickle=True)[\"dic\"].item()\n",
    "    ma = ModelApplier(vals)\n",
    "    all_results = []\n",
    "    for dataset in [train, test]:\n",
    "        print(\"running a dataset\")\n",
    "        square_losses = []\n",
    "        for i in range(len(dataset)):\n",
    "            if i % 10 == 0:\n",
    "                print(i)\n",
    "            square_losses.append(round_trip_loss(ma, dataset[i],basisphis,trajid, ts, xs, tau, alpha_z, beta_z,\n",
    "                                                 cs, scale_param))\n",
    "        square_losses = np.array(square_losses)\n",
    "        all_results.append(square_losses)\n",
    "    np.savez(f\"dmpmodels/intermediate_dmp_error_results_{latent_dim}.npz\",train=all_results[0], test=all_results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588737d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_results = []\n",
    "for latent_dim in range(1,17):\n",
    "    intermediate_results = np.load(f\"dmpmodels/intermediate_dmp_error_results_{latent_dim}.npz\")\n",
    "    valid_inds = intermediate_results[\"train\"][:,0] == latent_dim\n",
    "    #if np.sum(valid_inds) != 125:\n",
    "    #    continue\n",
    "    ld,train_aligned_loss, train_error, checkval, paramerror = np.mean(intermediate_results[\"train\"][valid_inds,],axis=0)\n",
    "    assert ld == latent_dim, \"check your latent_dim\"\n",
    "    #assert checkval == 62, \"should have indices ranging from 0 to 124\"\n",
    "    \n",
    "    valid_inds = intermediate_results[\"test\"][:,0] == latent_dim\n",
    "    latent_dim,test_aligned_loss, test_error, checkval, paramerror = np.mean(intermediate_results[\"test\"][valid_inds,],axis=0)\n",
    "    assert ld == latent_dim, \"check your latent_dim\"\n",
    "    #assert checkval == 62, \"should have indices ranging from 0 to 124\"\n",
    "    \n",
    "    final_results.append((latent_dim, np.sqrt(train_aligned_loss),\n",
    "                          np.sqrt(test_aligned_loss), \n",
    "                       np.sqrt(train_error), \n",
    "                          np.sqrt(test_error),\n",
    "                          paramerror\n",
    "                         ))\n",
    "final_results = np.array(final_results)\n",
    "np.savez(\"dmpmodels/dmp_results\",final_results=final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec9b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df37084",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(final_results[:,1])\n",
    "plt.plot(final_results[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd234bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
