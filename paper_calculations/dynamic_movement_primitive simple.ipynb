{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:09.557167Z",
     "start_time": "2023-10-23T13:25:07.928233Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import timewarp_lib.vector_timewarpers as vtw\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:09.584946Z",
     "start_time": "2023-10-23T13:25:09.559049Z"
    }
   },
   "outputs": [],
   "source": [
    "fulldat = np.load(\"../data/trainTest2DLetterACache.npz\")\n",
    "print(fulldat[\"train\"].shape)\n",
    "dat = fulldat[\"train\"]\n",
    "numdims = dat.shape[2]\n",
    "numts = dat.shape[1]\n",
    "numtrajs = dat.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "A DMP is a combination of a simple attractor-to-goal model and a forcing term.\n",
    "\n",
    "The forcing term is a function of phase `x` which (without external disturbances) is a direct function of time `t`.\n",
    "DMP models from the original Ijspeert paper\n",
    "(and the Learning Parametric Dynamic Movement... paper)\n",
    "have the forcing term affect velocity, not acceleration.\n",
    "\n",
    "The forcing term is a linear combination of kernel functions.\n",
    "The linear combination values is computed using locally weighted regression on \n",
    "the difference between the velocities that would exist at phase `x`\n",
    "if we just had the attractor-to-goal and no forcing term and no external velocities\n",
    "and the trajectory's actual velocity at phase `x`.\n",
    "Note that this does not find the minimum squared positional error optimal weights.\n",
    "Both because it is fitting velocities, not positions,\n",
    "and because it is using a local fitting procedure, not finding the optimal combination\n",
    "to reduce error across the whole trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase vs Time\n",
    "\n",
    "We don't use any of the flexibility that this additional complication allows,\n",
    "but for consistency with Ijspeert 2002,\n",
    "instead of functions of `t` for kernel functions, we use kernel functions that are \n",
    "a function of `x` according to the equations below.\n",
    "This additional complication is only useful if 1) you have external perturbations during motion\n",
    "and 2) you add some type of delaying/forcing function to how the phase evolves over time.\n",
    "$$ \\tau \\dot v = \\alpha_v (\\beta_v (g-x) - v) $$\n",
    "$$ \\tau \\dot x = v $$\n",
    "Since we don't use any of the additional flexibility that this equation allows, this is exactly the same as saying that\n",
    "$$ x = a e^{-\\phi_1 t} + b e^{-\\phi_2 t} $$\n",
    "Where the $\\phi$s are $\\frac 1 {2\\tau} (-\\alpha_v \\pm \\sqrt{\\alpha_v^2 - 4 \\alpha_v \\beta_v})$\n",
    "but since $\\alpha_v$ and $\\beta_v$ are chosen arbitrarily, we can instead just arbitrarily set \n",
    "$x = e^{-\\phi t}$ with $\\phi = \\log(100)$, making  $x_0 = 1$ and $\\dot x_0 = -\\phi$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:09.590030Z",
     "start_time": "2023-10-23T13:25:09.586697Z"
    }
   },
   "outputs": [],
   "source": [
    "tau = 1\n",
    "def phase_func(t):\n",
    "    phasefactor = np.log(100)\n",
    "    return np.exp(-t * phasefactor)\n",
    "# ts is the (scaled) timestamp\n",
    "ts = np.linspace(0,1,numts)\n",
    "# xs is the associated ``phase''\n",
    "xs = phase_func(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:09.729256Z",
     "start_time": "2023-10-23T13:25:09.592181Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(ts,xs)\n",
    "plt.xlabel(\"Time t\")\n",
    "plt.ylabel(\"Phase Value x\")\n",
    "plt.title(\"Phase is function of time\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:09.733482Z",
     "start_time": "2023-10-23T13:25:09.731115Z"
    }
   },
   "outputs": [],
   "source": [
    "N = numts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating forcing function from data\n",
    "\n",
    "Ijspeert 2002 explains:\n",
    "For learning from a given sample trajectory,\n",
    "characterized by a trajectory y demo (t), $\\dot y$ demo (t) and duration T , a supervised learning problem can be formulated with the target trajectory f target = τ $\\dot y$ demo − z demo\n",
    "for Eq.1 (right), where z demo is obtained by integrating Eq.1 (left) with y demo instead of y.\n",
    "\n",
    "The equations (Eq. 1) of Ijspeert 2002 are:\n",
    "$$ \\tau \\dot z = \\alpha_z (\\beta_z(g-y)-z) $$\n",
    "$$ \\tau \\dot y = z + f $$\n",
    "\n",
    "And the integration to solve for $z(t)$ is (using $s$ instead of $t$ for evaluation to avoid\n",
    "using same symbol $t$ for $dt$ and for limit of integration):\n",
    "$$ z(s) = \\frac 1 \\tau \\int_0^s \\alpha_z (\\beta_z (g-y_{demo}) - z) dt $$\n",
    "\n",
    "In this way, we're estimating the forcing function that \"must have existed\" during the training trajectory.\n",
    "\n",
    "The $z(t)$ we're calculating is hard to think about, but it can be thought of as \"the component of the velocity due to the goal attractor\". And the total velocity of the trajectory is that value $z$ plus the forcing term $f$ (times the time scaling term $\\tau$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:09.737368Z",
     "start_time": "2023-10-23T13:25:09.734848Z"
    }
   },
   "outputs": [],
   "source": [
    "# we choose these according to the suggested values in Ijspeert 2013 (didn't see suggestions in Ijspeert 2002)\n",
    "alpha_z = 25\n",
    "beta_z = alpha_z/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:09.744257Z",
     "start_time": "2023-10-23T13:25:09.738998Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform numeric integration on the training data to estimate\n",
    "# the component of the velocity of the trajectory that is due to the \"point-attractor\"\n",
    "# component of the trajectory model.\n",
    "# The rest of the velocity of the trajectory will be due to the \"forcing term\" f\n",
    "def numeric_integration(ydemos, ts, tau, g, alpha_z, beta_z):\n",
    "    # We perform numeric integration using small rectangles\n",
    "    # with width dt = step_size\n",
    "    # and height equal the value of 1/tau \\alpha_z (\\beta_z (g-y_{demo}) - z)\n",
    "    # evaluated at the left of the rectangle.\n",
    "    step_size = 0.00001\n",
    "    # start integrating at the first ts\n",
    "    t = ts[0]\n",
    "    # Since our y_demo is only known at certain places,\n",
    "    # keep track of the index i\n",
    "    # of the smallest ts that is larger than our current value of t\n",
    "    # when we want to compute y_delta at some t between ts[i-1] and ts[i]\n",
    "    # we do linear interpolation\n",
    "    i = 1\n",
    "    # keep track of our cumulative sum (which numerically estimates the integral)\n",
    "    z = 0\n",
    "    # keep track of the integral as it progresses.\n",
    "    # append a 0 to start, to avoid the fencepost problem (otherwise we have one fewer zs than ts)\n",
    "    zs = []\n",
    "    zs.append(z)\n",
    "    # compute the integral up to each of the t values in ts, appending the result to zs each time\n",
    "    while i < len(ts):\n",
    "        # keep on adding dt to t and summing the rectangle value until we hit the next value in ts\n",
    "        while i < len(ts) and t < ts[i]:\n",
    "            # we use linear interpolation to compute the ydemo value at the intermediate t value\n",
    "            interp_frac = (t - ts[i-1])/(ts[i] - ts[i-1])\n",
    "            y = (1-interp_frac) * ydemos[i-1] + interp_frac * ydemos[i]\n",
    "            # compute the area of the rectangle and add it to our cumulative integration.\n",
    "            # note that z appears on the left and right of this expression\n",
    "            z = z + (alpha_z * (beta_z * (g - y) - z)/tau * step_size)\n",
    "            t += step_size\n",
    "        zs.append(z)\n",
    "        i += 1\n",
    "    return np.array(zs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As a baseline check, just fit a PCA model of FTargets (don't worry about wss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:10.089908Z",
     "start_time": "2023-10-23T13:25:09.745642Z"
    }
   },
   "outputs": [],
   "source": [
    "# zdemos is (what) the velocity of the trajectory (would be, if just the attractor model)\n",
    "zdemos = []\n",
    "# ftargets is the imputed forcing function\n",
    "ftargets = []\n",
    "# both those are arrays of arrays, with indices according to\n",
    "# forcing velocity = ftargets[trajectory index][dimension]\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    ftargets.append([])\n",
    "    zdemos.append([])\n",
    "    for dim in range(2):\n",
    "        print(\"dim\",dim)        \n",
    "        ydemo = dat[i,:,dim]\n",
    "        plt.plot(ts,ydemo)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Position\")\n",
    "        ydemoprime = (ydemo[2:]-ydemo[:-2])/(ts[1]-ts[0])/2\n",
    "        ydemoprime = np.concatenate(((ydemo[1:2]-ydemo[:1])/(ts[1]-ts[0]),ydemoprime,(ydemo[-1:]-ydemo[-2:-1])/(ts[1]-ts[0])))\n",
    "\n",
    "        yzero = ydemo[0]\n",
    "        g = ydemo[-1]\n",
    "        \n",
    "        zdemo = numeric_integration(ydemo, ts, tau, g, alpha_z, beta_z)\n",
    "        ftarget = tau * ydemoprime - zdemo\n",
    "        ftargets[-1].append(ftarget)\n",
    "        zdemos[-1].append(zdemo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:10.232011Z",
     "start_time": "2023-10-23T13:25:10.091303Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# indices are [trajectory_index][dimension]\n",
    "plt.scatter(ts,zdemos[0][0],label=\"z\")\n",
    "plt.scatter(ts,-ftargets[0][0],label=\"f\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Velocity\")\n",
    "plt.title(\"Velocity from point attractor (z) and from forcing function (f)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct trajectory\n",
    "From the weights we reconstructed the forcing function `fitted_f`.\n",
    "We can then integrate and simulate forward to see what the point-attractor and forcing function together would cause the\n",
    "trajectory to actually be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:10.239040Z",
     "start_time": "2023-10-23T13:25:10.235211Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate(fitted_f, ts, g, alpha_z, beta_z, yzero):\n",
    "    step_size = 0.00001\n",
    "    i = 0\n",
    "    t = ts[i]\n",
    "    ys = [] # position\n",
    "    zs = [] # velocity\n",
    "    y = yzero\n",
    "    z = 0\n",
    "    t = 0\n",
    "    while i < len(ts):\n",
    "        while i < len(ts) and t < ts[i]:\n",
    "            interp_frac = (t - ts[i-1])/(ts[i] - ts[i-1])\n",
    "            f = (1-interp_frac) * fitted_f[i-1] + interp_frac * fitted_f[i]\n",
    "            z += alpha_z * (beta_z * (g - y) - z)/tau * step_size\n",
    "            y += (z + f)/tau * step_size\n",
    "            t += step_size\n",
    "        ys.append(y)\n",
    "        zs.append(z)\n",
    "        i += 1\n",
    "    return (np.array(ys), np.array(zs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:10.700924Z",
     "start_time": "2023-10-23T13:25:10.240521Z"
    }
   },
   "outputs": [],
   "source": [
    "for dim in range(2):\n",
    "    # get the trajectory dimension start and end for scaling\n",
    "    ydemo = dat[0,:,dim]\n",
    "    yzero = ydemo[0]\n",
    "    g = ydemo[-1]\n",
    "    fitted_fs = ftargets\n",
    "    recon_ys,recon_zs = simulate(fitted_fs[0][dim],ts,g,alpha_z,beta_z, yzero)\n",
    "    plt.plot(ts, ydemo,   label=\"actual\")\n",
    "    plt.plot(ts, recon_ys,label=\"recon\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Position\")\n",
    "    plt.title(f\"Reconstruction Dim {dim}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Function to Compute DMP parameterization of training trajectory\n",
    "Write a single function to compute the DMP parameters computed in the steps above\n",
    "(and confirm you get the same answer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:10.706744Z",
     "start_time": "2023-10-23T13:25:10.702268Z"
    }
   },
   "outputs": [],
   "source": [
    "# traj should be 2d\n",
    "def compute_dmp_parameterization(traj, ts, tau, alpha_z, beta_z):\n",
    "    assert len(traj.shape) == 2, \"traj should be 2d...just send in one trajectory\"\n",
    "    # reset the trajectory to start at (0,0)\n",
    "    # and save the actual starting position to start_offset variables\n",
    "    start_offset_0, start_offset_1 = traj[0]\n",
    "    ydemos = traj-traj[0:1]\n",
    "    result_dictionary = {}\n",
    "    for dim in range(2):\n",
    "        ydemo = ydemos[:,dim]\n",
    "        ydemoprime = (ydemo[2:]-ydemo[:-2])/(ts[1]-ts[0])/2\n",
    "        ydemoprime = np.concatenate(((ydemo[1:2]-ydemo[:1])/(ts[1]-ts[0]),ydemoprime,(ydemo[-1:]-ydemo[-2:-1])/(ts[1]-ts[0])))\n",
    "        yzero = ydemo[0]\n",
    "        assert yzero == 0 , \"We shifted training trajectory to start at 0\"\n",
    "        g = ydemo[-1]\n",
    "        zdemo = numeric_integration(ydemo, ts, tau, g, alpha_z, beta_z)\n",
    "        ftarget = tau * ydemoprime - zdemo\n",
    "        result_dictionary[f\"ftarget_{dim}\"] = ftarget\n",
    "        result_dictionary[f\"g_{dim}\"] = g\n",
    "    result_dictionary[f\"start_offset_0\"] = start_offset_0\n",
    "    result_dictionary[f\"start_offset_1\"] = start_offset_1\n",
    "    return result_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:10.918103Z",
     "start_time": "2023-10-23T13:25:10.707797Z"
    }
   },
   "outputs": [],
   "source": [
    "trained_model = compute_dtw_parameterization(dat[0,:,:], ts, tau, alpha_z, beta_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:10.982813Z",
     "start_time": "2023-10-23T13:25:10.919466Z"
    }
   },
   "outputs": [],
   "source": [
    "# check to make sure that our single-function-solution\n",
    "# gets the same answer as our step-by-step approach above\n",
    "for dim in range(2):\n",
    "    np.testing.assert_almost_equal(trained_model[f\"ftarget_{dim}\"],ftargets[0][dim])\n",
    "    np.testing.assert_almost_equal(trained_model[f\"g_{dim}\"],dat[0,-1,dim] - dat[0,0,dim])\n",
    "    np.testing.assert_almost_equal(trained_model[f\"start_offset_{dim}\"],dat[0,0,dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single function to reconstruct trajectory from parameter vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:10.986695Z",
     "start_time": "2023-10-23T13:25:10.984086Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_dic_to_parameter_vector(dic):\n",
    "    params = np.concatenate((dic[\"ftarget_0\"],dic[\"ftarget_1\"],\n",
    "                                    [dic[\"start_offset_0\"],dic[\"start_offset_1\"],\n",
    "                                    dic[\"g_0\"],dic[\"g_1\"]]\n",
    "                                   )).reshape(1,-1)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:10.991483Z",
     "start_time": "2023-10-23T13:25:10.987951Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_parameter_vector(parameter_vector, N):\n",
    "    ftarget_0 = parameter_vector[:N]\n",
    "    ftarget_1 = parameter_vector[N:2*N]\n",
    "    start_offset_0,start_offset_1,g_0,g_1 = parameter_vector[2*N:]\n",
    "    \n",
    "    positions = []\n",
    "    for g, ftarget, start_offset in [(g_0,ftarget_0,start_offset_0), (g_1,ftarget_1,start_offset_1)]:\n",
    "        yzero=0 # centered training data all starts at zero\n",
    "        fitted_f = ftarget\n",
    "        # ys = position, zs = velocity\n",
    "        ys,zs = simulate(fitted_f,ts,g,alpha_z,beta_z,yzero)\n",
    "        positions.append(ys + start_offset)\n",
    "    positions=np.array(positions).T\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:11.234761Z",
     "start_time": "2023-10-23T13:25:10.992805Z"
    }
   },
   "outputs": [],
   "source": [
    "param_vect = convert_dic_to_parameter_vector(trained_model)[0]\n",
    "positions = decode_parameter_vector(param_vect, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:11.480269Z",
     "start_time": "2023-10-23T13:25:11.236046Z"
    }
   },
   "outputs": [],
   "source": [
    "# check to make sure that our single-function-solution\n",
    "# gets the same answer as our step-by-step approach above\n",
    "for dim in range(2):\n",
    "    # get the trajectory dimension start and end for scaling\n",
    "    ydemo = dat[0,:,dim]\n",
    "    yzero = ydemo[0]\n",
    "    g = ydemo[-1]\n",
    "    recon_ys,recon_zs = simulate(fitted_fs[0][dim],ts,g,alpha_z,beta_z, yzero)\n",
    "    np.testing.assert_almost_equal(positions[:,dim],recon_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a linear PCA model on the dmp parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:11.489232Z",
     "start_time": "2023-10-23T13:25:11.481662Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_dmp_parameter_model(training_data, latent_dim, scale_last_four_dims):\n",
    "    num_trajs, num_channels = training_data.shape\n",
    "    print(f\"we have num_trajs:{num_trajs}, num_channels:{num_channels}\")\n",
    "    scaled_params = np.copy(training_data)\n",
    "    \n",
    "    scaled_params[:,-4:] = scaled_params[:,-4:] * scale_last_four_dims\n",
    "    N = int((num_channels-4)/2)\n",
    "    \n",
    "    \n",
    "\n",
    "    # compute and remove the mean parameter values (mean parameter values is of shape (num_channels))\n",
    "    mean_params = np.mean(scaled_params, axis=0)\n",
    "    assert len(mean_params) == N*2 + 4, \"mean parameter should be valid parameterization\"\n",
    "    centered_params = scaled_params - mean_params[np.newaxis,:]\n",
    "\n",
    "    # compute the PCA model using SVD\n",
    "    u, s, vt = np.linalg.svd(centered_params)\n",
    "    \n",
    "\n",
    "    # keep only the first latent_dim number of dimensions\n",
    "    if len(s) < latent_dim:\n",
    "        raise Exception(f\"You can't build a model of dimension {latent_dim} on a dataset with dimensionality {len(s)}\")\n",
    "\n",
    "    # the singular values written in matrix form\n",
    "    smat = np.diag(s[:latent_dim])\n",
    "    smatinv = np.diag(1./s[:latent_dim])\n",
    "    # the first latent_dim directions of variation\n",
    "    basis_vectors = vt[:latent_dim,:]\n",
    "    # given a trajectory, use this to find its value in the latent space \n",
    "    # note that the dimensions of the embedding matrix are (latent_dim, nt*nc)\n",
    "    embedding_matrix = smatinv @ basis_vectors \n",
    "    # note that we include the s factor here so that our latent space is roughly the unit gaussian ball :brain:\n",
    "    # note further that the dimensions of the reconstruction matrix are also (latent_dim, nt*nc)\n",
    "    reconstruction_matrix = smat @ basis_vectors\n",
    "    \n",
    "    # store the basis_vectors too,\n",
    "    # so that we can avoid numerical imprecision from smatinv * smat\n",
    "\n",
    "    return {\n",
    "      \"num_trajs\" : num_trajs,\n",
    "      \"num_channels\" : num_channels,\n",
    "      \"latent_dim\" : latent_dim,\n",
    "      \"mean_params\" : mean_params,\n",
    "      \"embedding_matrix\" : embedding_matrix,\n",
    "      \"reconstruction_matrix\" : reconstruction_matrix,\n",
    "      \"basis_vectors\" : basis_vectors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:11.495802Z",
     "start_time": "2023-10-23T13:25:11.490825Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a ModelApplier object based purely on a\n",
    "# directory containing model information\n",
    "class ModelApplier(object):\n",
    "    def __init__(self, modeldata):\n",
    "        self.num_trajs = modeldata[\"num_trajs\"]\n",
    "        self.num_channels = modeldata[\"num_channels\"]\n",
    "        self.latent_dim = modeldata[\"latent_dim\"]\n",
    "        self.mean_params = modeldata[\"mean_params\"]\n",
    "        self.embedding_matrix = modeldata[\"embedding_matrix\"]\n",
    "        self.reconstruction_matrix = modeldata[\"reconstruction_matrix\"]\n",
    "        self.basis_vectors = modeldata[\"basis_vectors\"]\n",
    "\n",
    "    # The input data should be of the shape\n",
    "    # (num_apply_trajs, apply_latent_dim)\n",
    "    def recon(self, data):\n",
    "        num_apply_trajs, apply_latent_dim = data.shape\n",
    "        if apply_latent_dim != self.latent_dim:\n",
    "              raise Exception(f\"The number of latent dim coordinates given: {apply_latent_dim} was not the same as the {self.latent_dim} expected by the model\")\n",
    "\n",
    "        # compute the linear combination of basis vectors \n",
    "        traj_offset_vectors = data @ self.reconstruction_matrix\n",
    "        mean_vector = self.mean_params.reshape(1,self.num_channels)\n",
    "        # add back the mean vector\n",
    "        traj_vectors = traj_offset_vectors + mean_vector\n",
    "        # reshape to the official standard expected shape\n",
    "        # (num_apply_trajs, num_timesteps, num_channels)\n",
    "        result_trajs = traj_vectors.reshape(num_apply_trajs, self.num_channels)\n",
    "        return result_trajs\n",
    "\n",
    "    # The input data should be of the shape\n",
    "    # (num_apply_trajs, self.num_channels)\n",
    "    # The output latent dimensions are of the shape\n",
    "    # (num_apply_trajs, self.latent_dim)\n",
    "    def embed(self, data):\n",
    "        num_apply_trajs, apply_num_channels = data.shape\n",
    "        assert apply_num_channels == self.num_channels, \"channels must match for PCA\"\n",
    "\n",
    "        flattened_trajs = data\n",
    "        mean_vector = self.mean_params.reshape(1,self.num_channels)\n",
    "        centered_trajs = flattened_trajs - mean_vector\n",
    "        # the dimensions of the embedding matrix are (latent_dim, nc)\n",
    "        latent_vals_mat = centered_trajs @ self.embedding_matrix.T\n",
    "        return(latent_vals_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:11.500714Z",
     "start_time": "2023-10-23T13:25:11.497277Z"
    }
   },
   "outputs": [],
   "source": [
    "scale_param = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:11.984550Z",
     "start_time": "2023-10-23T13:25:11.502182Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# trivial check that a 1D model on 2 parameters gives perfect reconstructions of that model\n",
    "two_dicts = [compute_dmp_parameterization(dat[i,:,:], ts, tau, alpha_z, beta_z) for i in range(2)]\n",
    "two_vectors = np.concatenate([convert_dic_to_parameter_vector(two_dicts[i]) for i in range(2)],axis=0)\n",
    "       \n",
    "# compute dmp parameters for two trajectories\n",
    "\n",
    "perf_model_params = train_dmp_parameter_model(two_vectors, 1, scale_param)\n",
    "perf_model = ModelApplier(perf_model_params)\n",
    "\n",
    "np.testing.assert_almost_equal(np.sum(np.square(perf_model.basis_vectors[0])),1)\n",
    "\n",
    "scaled_two_vectors = np.copy(two_vectors)\n",
    "scaled_two_vectors[:,-4:] *= scale_param\n",
    "\n",
    "np.testing.assert_almost_equal(perf_model.recon(perf_model.embed(scaled_two_vectors)), scaled_two_vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given a PCA model on training data, compute round-trip test error\n",
    "Round-trip test error computed with and without DTW temporal alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:11.988251Z",
     "start_time": "2023-10-23T13:25:11.985968Z"
    }
   },
   "outputs": [],
   "source": [
    "test_time_dtw_vector_timewarper = vtw.DTWVectorTimewarper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:11.996254Z",
     "start_time": "2023-10-23T13:25:11.989675Z"
    }
   },
   "outputs": [],
   "source": [
    "def round_trip_loss(ma, traj, logging_index, ts, tau, alpha_z, beta_z, scale_last_four_dims):\n",
    "    assert len(traj.shape) == 2, \"pass in one traj at a time\"\n",
    "    num_ts, channels = traj.shape\n",
    "    N = num_ts\n",
    "    assert channels == 2, \"2 channels for handwriting\"\n",
    "    dic = compute_dtw_parameterization(traj, ts, tau, alpha_z, beta_z)\n",
    "    params = np.concatenate((dic[\"ftarget_0\"],dic[\"ftarget_1\"],\n",
    "                                    [dic[\"start_offset_0\"],dic[\"start_offset_1\"],\n",
    "                                    dic[\"g_0\"],dic[\"g_1\"]]\n",
    "                                   )).reshape(1,-1)\n",
    "    params_copy = np.copy(params)\n",
    "    params_copy[:,-4:] = scale_last_four_dims * params_copy[:,-4:]\n",
    "    \n",
    "    new_parameters = ma.recon(ma.embed(params_copy))\n",
    "    recon_scaled = np.copy(new_parameters)\n",
    "    new_parameters[0][-4:] = new_parameters[0][-4:]/scale_last_four_dims\n",
    "    pos = decode_parameter_vector(new_parameters[0], N)\n",
    "    recon_train = pos.reshape((1,num_ts, channels))\n",
    "    train = traj.reshape((1,num_ts, channels))\n",
    "    #plt.plot(train[0,:,0],train[0,:,1],label=\"train\")\n",
    "    #plt.plot(recon_train[0,:,0],recon_train[0,:,1],label=\"recon\")\n",
    "    #plt.show()\n",
    "    #\n",
    "    ##plt.plot(params[0])\n",
    "    #plt.plot(params_copy[0])\n",
    "    #plt.plot(recon_scaled[0])\n",
    "    #plt.show()\n",
    "    \n",
    "    train_dtw_recon, train_dtw_actual = test_time_dtw_vector_timewarper.timewarp_first_and_second(\n",
    "        torch.tensor(recon_train,dtype=torch.float), \n",
    "        torch.tensor(train,dtype=torch.float))\n",
    "    train_aligned_loss = (\n",
    "        nn.functional.mse_loss(train_dtw_recon, train_dtw_actual, reduction=\"sum\").detach().numpy()\n",
    "        / (num_ts))\n",
    "    train_error = np.sum(np.square(recon_train - train))/(num_ts)\n",
    "    \n",
    "    params_error = np.sqrt(np.sum(np.square(params_copy - recon_scaled)))\n",
    "    squareresults = (ma.latent_dim, train_aligned_loss, train_error, logging_index, params_error)\n",
    "    return squareresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:25:12.832033Z",
     "start_time": "2023-10-23T13:25:11.997624Z"
    }
   },
   "outputs": [],
   "source": [
    "# we don't have a perfect round-trip loss because\n",
    "# the dmp model applied by itself still has some error.\n",
    "\n",
    "#round-trip error for a perfect linear model that exactly reconstructs dmp parameters\n",
    "trajid = 0\n",
    "(_,_,train_err,_,_) = round_trip_loss(perf_model, dat[trajid],trajid, ts, tau, alpha_z, beta_z, scale_param)\n",
    "\n",
    "# error for the dmp model\n",
    "combined_squared_error = 0\n",
    "for dim in range(2):\n",
    "    # get the trajectory dimension start and end for scaling\n",
    "    ydemo = dat[0,:,dim]\n",
    "    yzero = ydemo[0]\n",
    "    g = ydemo[-1]\n",
    "    recon_ys,recon_zs = simulate(fitted_fs[0][dim],ts,g,alpha_z,beta_z, yzero)\n",
    "    combined_squared_error += np.mean((recon_ys - ydemo)**2)\n",
    "    \n",
    "# they tie out\n",
    "np.testing.assert_almost_equal(train_err, combined_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:38:00.947627Z",
     "start_time": "2023-10-23T13:25:12.833308Z"
    }
   },
   "outputs": [],
   "source": [
    "important_results_object = []\n",
    "for i in range(len(dat)):\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "    result_dictionary = compute_dmp_parameterization(dat[i], ts, tau, alpha_z, beta_z)\n",
    "    important_results_object.append(result_dictionary)\n",
    "    \n",
    "parameters_vector = np.array([np.concatenate((dic[\"ftarget_0\"],dic[\"ftarget_1\"],\n",
    "                                    [dic[\"start_offset_0\"],dic[\"start_offset_1\"],\n",
    "                                    dic[\"g_0\"],dic[\"g_1\"]]\n",
    "                                   ))\n",
    "                     for dic in important_results_object])\n",
    "\n",
    "np.save(\"dmpmodels/parameters_vector_dmps_simple.npy\", parameters_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T13:38:09.045892Z",
     "start_time": "2023-10-23T13:38:00.949039Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters_vector = np.load(\"dmpmodels/parameters_vector_dmps_simple.npy\")\n",
    "for latent_dim in range(1,17):\n",
    "    vals = train_dmp_parameter_model(parameters_vector,latent_dim, scale_last_four_dims=scale_param)\n",
    "    np.savez(f\"dmpmodels/parametric_dmp_{latent_dim}_simple.npz\", dic=vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T14:14:48.033739Z",
     "start_time": "2023-10-23T13:43:32.919345Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATAFILE=f\"../data/trainTest2DLetterARescaled.npz\"\n",
    "data = np.load(DATAFILE)\n",
    "test = data[\"test\"]\n",
    "train = data[\"train\"]\n",
    "num_trains, num_ts, channels = train.shape\n",
    "num_tests, num_ts, channels = test.shape\n",
    "\n",
    "for latent_dim in range(1,17):\n",
    "    print(latent_dim)\n",
    "    vals = np.load(f\"dmpmodels/parametric_dmp_{latent_dim}_simple.npz\",allow_pickle=True)[\"dic\"].item()\n",
    "    ma = ModelApplier(vals)\n",
    "    all_results = []\n",
    "    for dataset in [train, test]:\n",
    "        print(\"running a dataset\")\n",
    "        square_losses = []\n",
    "        for i in range(len(dataset)):\n",
    "            if i % 10 == 0:\n",
    "                print(i)\n",
    "            square_losses.append(round_trip_loss(ma, dataset[i],trajid, ts, tau, alpha_z, beta_z,\n",
    "                                                 scale_param))\n",
    "        square_losses = np.array(square_losses)\n",
    "        all_results.append(square_losses)\n",
    "    np.savez(f\"dmpmodels/intermediate_dmp_error_results_{latent_dim}_simple.npz\",train=all_results[0], test=all_results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T17:04:54.297594Z",
     "start_time": "2023-10-23T17:04:54.271846Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_results = []\n",
    "for latent_dim in range(1,17):\n",
    "    intermediate_results = np.load(f\"dmpmodels/intermediate_dmp_error_results_{latent_dim}_simple.npz\")\n",
    "    valid_inds = intermediate_results[\"train\"][:,0] == latent_dim\n",
    "    #if np.sum(valid_inds) != 125:\n",
    "    #    continue\n",
    "    ld,train_aligned_loss, train_error, checkval, paramerror = np.mean(intermediate_results[\"train\"][valid_inds,],axis=0)\n",
    "    assert ld == latent_dim, \"check your latent_dim\"\n",
    "    #assert checkval == 62, \"should have indices ranging from 0 to 124\"\n",
    "    \n",
    "    valid_inds = intermediate_results[\"test\"][:,0] == latent_dim\n",
    "    latent_dim,test_aligned_loss, test_error, checkval, paramerror = np.mean(intermediate_results[\"test\"][valid_inds,],axis=0)\n",
    "    assert ld == latent_dim, \"check your latent_dim\"\n",
    "    #assert checkval == 62, \"should have indices ranging from 0 to 124\"\n",
    "    \n",
    "    final_results.append((latent_dim, np.sqrt(train_aligned_loss),\n",
    "                          np.sqrt(test_aligned_loss), \n",
    "                       np.sqrt(train_error), \n",
    "                          np.sqrt(test_error)\n",
    "                         ))\n",
    "final_results = np.array(final_results)\n",
    "np.save(\"dmpmodels/dmp_results_simple\",final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T17:04:54.675520Z",
     "start_time": "2023-10-23T17:04:54.672096Z"
    }
   },
   "outputs": [],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T14:29:02.781267Z",
     "start_time": "2023-10-23T14:29:02.663077Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(final_results[:,1],label=\"Train\")\n",
    "plt.plot(final_results[:,2], label=\"Test\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
